{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c3f937",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91049b",
   "metadata": {},
   "source": [
    "## Data cleaning for text data\n",
    "\n",
    "The data used in this part is text data from Twitter API. \n",
    "You can find the raw data here:\\\n",
    "https://github.com/anly501/anly-501-project-WilliamChuFCB/tree/main/data/raw_data\n",
    "\n",
    "The data mentioned above is named “tweet_freeway.csv” and “tweet_metro.csv” respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c194c96",
   "metadata": {},
   "source": [
    "First import necessary packages and read the raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "694c34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# GET STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# INITALIZE STEMMER+LEMITZIZER+SIA\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c4d9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('tweet_freeway.csv') \n",
    "df2=pd.read_csv('tweet_metro.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d985558",
   "metadata": {},
   "source": [
    "Let's take a quick look at the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "718a5659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@RealDeniseWelch Wake up Ms holier than thou.  Harry is cashing in off Diana &amp; the only time Migraine doesn't lie is when she's asleep.\",\n",
       " 'I hate the 5 freeway so much',\n",
       " 'Westbound 91 Freeway closed over weekend https://t.co/ynRCJmjh72 https://t.co/Exo4XAIqO1',\n",
       " 'MULTI FR24 CIRCLING ALERT : At time Sat Dec 10 00:09:05 2022 #N2824Y was likely to be circling at FL34 1nm from LG… https://t.co/KhDXGLDd3x',\n",
       " 'Freeway Fallguy #Juegos #Autos https://t.co/iNBoLT0rmp',\n",
       " 'if no control ever comes on when i’m driving i’ll swerve off the freeway',\n",
       " '@TideorDieChick These niggas had a blockade set up on the freeway and did donuts!',\n",
       " 'I’m stuck in the freeway and I gotta go home and pack 😩',\n",
       " 'Freeway tells me “drive hammered, get nailed.” Uh oh']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df1.iloc[10:19,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc6bbe34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@My_Metro The one next due (which is meant to be going to the coast based on your last reply) is showing as airport… https://t.co/aCxaxVWU7o',\n",
       " '@My_Metro Thanks!',\n",
       " '@socialepfo @PMOIndia @byadavbjp @Rameswar_Teli @LabourMinistry @mygovindia @MIB_India @PIB_India @AmritMahotsav In… https://t.co/OQW2sDWKtx',\n",
       " 'If you ever get time do notice that the  frequency of metro towards Noida are less as compared to metro towards Dwa… https://t.co/DwzX97RT5f',\n",
       " 'RT @drdpgoel: On board the Nagpur Metro, Hon’ble prime minister Shri @narendramodi ji interacted with students, those from the start up sec…',\n",
       " '\"#MetroInDino will have 4 segments, each story going on to be linked with one another in the end. #AnuragBasu had d… https://t.co/UxvydjKtAv',\n",
       " '@jingyi25510683 Five players banned from World Snooker Tour over match-fixing allegations https://t.co/KBIIthv8Mm via @Metro_Sport',\n",
       " \"@Olegrio75499074 The Metro Police are a bunch of 🌈 snowflakes, they don't charge, and the attackers know it...\",\n",
       " 'Kaiser Permanente supports metro Atlanta communities to end homelessness(Sponsored Content by @KPGeorgia) https://t.co/FM8AVGJR25']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df2.iloc[10:19,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df1234b",
   "metadata": {},
   "source": [
    "It is obvious that this raw data is difficult for doing further analysis due to unwanted characters and inconsistent format. Thus, we need to use a series of data cleaning method to polish these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeea8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "    #FILTER OUT UNWANTED CHAR\n",
    "    new_text=\"\"\n",
    "    keep=\" abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "    for character in text:\n",
    "        if character.lower() in keep:\n",
    "            new_text+=character.lower()\n",
    "        else: \n",
    "            new_text+=\" \"\n",
    "    text=new_text\n",
    "\n",
    "    #FILTER OUT UNWANTED WORDS\n",
    "    new_text=\"\"\n",
    "    for word in nltk.tokenize.word_tokenize(text):\n",
    "        if word not in nltk.corpus.stopwords.words('english'):\n",
    "            #lemmatize \n",
    "            tmp=lemmatizer.lemmatize(word)\n",
    "            word=tmp\n",
    "            if len(word)>1:\n",
    "                if word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n",
    "                    #remove the last space\n",
    "                    new_text=new_text[0:-1]+word+\" \"\n",
    "                else: #add a space\n",
    "                    new_text+=word.lower()+\" \"\n",
    "    text=new_text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b415ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_free_list=list(df1.iloc[:,0])\n",
    "sh_mtr_list=list(df2.iloc[:,0])\n",
    "for i in range(len(sh_free_list)):\n",
    "    sh_free_list[i]=clean_string(sh_free_list[i])\n",
    "for i in range(len(sh_mtr_list)):\n",
    "    sh_mtr_list[i]=clean_string(sh_mtr_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef617a7",
   "metadata": {},
   "source": [
    "After the process above, we have filtered out unwanted characters, spaces and stopwords. Have a look at this data now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffaf1298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['realdenisewelch wake holier thou harry cashing diana amp time migraine lie asleep',\n",
       " 'hate freeway much',\n",
       " 'westbound 91 freeway closed weekend http co ynrcjmjh72 http co exo4xaiqo1',\n",
       " 'multi fr24 circling alert time sat dec 10 00 09 05 2022 n2824y likely circling fl34 1nm lg http co khdxgldd3x',\n",
       " 'freeway fallguy juegos auto http co inbolt0rmp',\n",
       " 'control ever come driving swerve freeway',\n",
       " 'tideordiechick nigga blockade set freeway donut',\n",
       " 'stuck freeway got ta go home pack',\n",
       " 'freeway tell drive hammered get nailed uh oh']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_free_list[10:19]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a99adf",
   "metadata": {},
   "source": [
    "We can see that the data is much cleaner now. We only keep English characters and numbers, and we also remove those words without actual meaning, like preposition. Perhaps you will find this data more difficult to understand, but it is absolutely much easier for the computer to comprehend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d51aa8",
   "metadata": {},
   "source": [
    "Next, we arrange labels to the data and combine tweets about freeway and metro into one dataset. This is important for further analysis, like some supervised classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5b6f311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    label\n",
      "0  boenau rfsdfhsfbhwsfgb least ca expressway mig...  freeway\n",
      "1  zacakamadu sudden urge go san jose talk cultur...  freeway\n",
      "2        alinaaaziz yes girl freeway system wack lol  freeway\n",
      "3  multi adsbx circling alert time sat dec 10 18 ...  freeway\n",
      "4  ghettosmosh almost crashed fucking whip freewa...  freeway\n",
      "                                                  text  label\n",
      "876  mirazi8 enquiry case found mentioned address t...  metro\n",
      "877  drishtisharma02 even highlighted action taken ...  metro\n",
      "878  rt vjsubhash01 bos return chennai metro carry ...  metro\n",
      "879  rt rweingarten yesterday new mexico family amp...  metro\n",
      "880  stephedger one train departed next train due l...  metro\n"
     ]
    }
   ],
   "source": [
    "tmp1=[]\n",
    "for i in range(0,len(sh_free_list)):\n",
    "    tmp1.append([sh_free_list[i],\"freeway\"])\n",
    "df1=pd.DataFrame(tmp1)\n",
    "df1=df1.rename(columns={0: \"text\", 1: \"label\"})\n",
    "\n",
    "tmp2=[]\n",
    "for i in range(0,len(sh_mtr_list)):\n",
    "    tmp2.append([sh_mtr_list[i],\"metro\"])\n",
    "df2=pd.DataFrame(tmp2)\n",
    "df2=df2.rename(columns={0: \"text\", 1: \"label\"})\n",
    "\n",
    "df=pd.concat([df1,df2])\n",
    "print(df.head())\n",
    "print(df.tail())\n",
    "df.to_csv('free_mtr_text_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f394aa",
   "metadata": {},
   "source": [
    "The data is ideal for further analysis now. You can access to this cleaned data here:\\\n",
    "https://github.com/anly501/anly-501-project-WilliamChuFCB/tree/main/data/cleaned_data\n",
    "\n",
    "The name of this dataset is \"free_mtr_text_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133ba7a",
   "metadata": {},
   "source": [
    "In order to use algorithms to do classification on this dataset, we still need some steps to make the data more comprehensible for the computer. First, convert the string labels to integer labels using 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ff9d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0 : label = freeway\n",
      "index = 1 : label = metro\n",
      "number of text chunks =  3465\n",
      "--------------------\n",
      "example of the data:\n",
      "['boenau rfsdfhsfbhwsfgb least ca expressway might limited access still grade inte http co dibmsr9b5b', 'zacakamadu sudden urge go san jose talk cultural impact song unfortunately san jo http co nujnntbhx0', 'alinaaaziz yes girl freeway system wack lol']\n"
     ]
    }
   ],
   "source": [
    "#CONVERT FROM STRING LABELS TO INTEGERS \n",
    "labels=[]\n",
    "y1=[]\n",
    "for label in df[\"label\"]:\n",
    "    if label not in labels:\n",
    "        labels.append(label)\n",
    "        print(\"index =\",len(labels)-1,\": label =\",label)\n",
    "    for i in range(0,len(labels)):\n",
    "        if(label==labels[i]):\n",
    "            y1.append(i)\n",
    "y1=np.array(y1)\n",
    "\n",
    "# CONVERT DF TO LIST OF STRINGS \n",
    "textdata=df[\"text\"].to_list()\n",
    "\n",
    "print(\"number of text chunks = \",len(textdata))\n",
    "print(\"--------------------\")\n",
    "print(\"example of the data:\")\n",
    "print(textdata[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb42cc",
   "metadata": {},
   "source": [
    "Then, vectorize the data and transform it into onehot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e980be7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of onehot matrix:\n",
      "(3465, 1332)\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE COUNT VECTORIZER\n",
    "vectorizer=CountVectorizer(min_df=5)   \n",
    "\n",
    "# RUN COUNT VECTORIZER ON OUR COURPUS \n",
    "vec=vectorizer.fit_transform(textdata)   \n",
    "dense=np.array(vec.todense())\n",
    "\n",
    "#CONVERT TO ONE-HOT VECTORS\n",
    "maxs=np.max(dense,axis=0)\n",
    "onehot=np.ceil(dense/maxs)\n",
    "\n",
    "# DOUBLE CHECK \n",
    "print(\"shape of onehot matrix:\")\n",
    "print(onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b684f1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71ccad",
   "metadata": {},
   "source": [
    "We can see that onehot matrix is absolutely a sparse matrix, where most values in this matrix is 0. Every row of this matrix represents a single text chunk while every column repersents a word. \"1\" means the word represented by the column exists in the text chunk represented by the row. By contrast, \"0\" means the word does not exist in the text chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a6840d",
   "metadata": {},
   "source": [
    "## Data cleaning for record data\n",
    "\n",
    "This data comes from statsmodels.org. It contains information of passengers’ different choice of travel mode.\n",
    "\n",
    "The meanings of variables are as follows:\\\n",
    "TTME - terminal waiting time\\\n",
    "INVC - in vehicle cost for all stages\\\n",
    "INVT - travel time (in-vehicle time) for all stages\\\n",
    "GC - generalized cost measure:invc+(invt*value of travel time savings)\\\n",
    "HINC - household income\\\n",
    "PSIZE - traveling group size\n",
    "\n",
    "As for the target variable \"MODE\", it is relatively complex. Every four rows represent for one passenger and each row of these four represents \"air\", \"train\", \"bus\", \"car\" respectively from top to bottom. The \"MODE\" variable is similar to an one-hot variable. When \"MODE\" equals to 1, it means this passenger choose the travel mode represented by the row where this 1 is located in.\n",
    "\n",
    "The target of cleaning this data is to make it more comprehensible and easy for further analysis.\n",
    "\n",
    "You can find the raw data here:\\\n",
    "https://github.com/anly501/anly-501-project-WilliamChuFCB/tree/main/data/raw_data\n",
    "\n",
    "The name of this file is \"travel.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800d3dc",
   "metadata": {},
   "source": [
    "Read from csv file and have a look at the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "05369adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MODE</th>\n",
       "      <th>TTME</th>\n",
       "      <th>INVC</th>\n",
       "      <th>INVT</th>\n",
       "      <th>GC</th>\n",
       "      <th>HINC</th>\n",
       "      <th>PSIZE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>59</td>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>372</td>\n",
       "      <td>71</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "      <td>417</td>\n",
       "      <td>70</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>180</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>58</td>\n",
       "      <td>68</td>\n",
       "      <td>68</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>354</td>\n",
       "      <td>84</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>25</td>\n",
       "      <td>399</td>\n",
       "      <td>85</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>255</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>115</td>\n",
       "      <td>125</td>\n",
       "      <td>129</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>98</td>\n",
       "      <td>892</td>\n",
       "      <td>195</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MODE  TTME  INVC  INVT   GC  HINC  PSIZE \n",
       "0     0    69    59   100   70    35       1\n",
       "1     0    34    31   372   71    35       1\n",
       "2     0    35    25   417   70    35       1\n",
       "3     1     0    10   180   30    35       1\n",
       "4     0    64    58    68   68    30       2\n",
       "5     0    44    31   354   84    30       2\n",
       "6     0    53    25   399   85    30       2\n",
       "7     1     0    11   255   50    30       2\n",
       "8     0    69   115   125  129    40       1\n",
       "9     0    34    98   892  195    40       1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=pd.read_csv('travel.csv')\n",
    "df3.iloc[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55418306",
   "metadata": {},
   "source": [
    "First, I notice that the data rows represent for \"car\" is quite different from other rows. For example, the \"TTME\" variable is always 0 for \"car\" and it is greater than 0 for others. This might lead to data skew, so I choose to remove all the rows related to \"car\". The remained rows contain three categories: \"air\", \"bus\" and \"train\". All of these three are public transportation and it makes sense to do analysis among these three travel mode. Then, transform the onehot variable \"MODE\" into a typical class label \"choice\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89a1d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"choice\"]=[1,2,3,4]*210\n",
    "\n",
    "df3_cleaned=df3.loc[df3[\"MODE\"]==1,]\n",
    "df3_cleaned=df3_cleaned.loc[df3[\"choice\"]!=4,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbf3bc",
   "metadata": {},
   "source": [
    "Remove the old variable \"MODE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e8601c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TTME</th>\n",
       "      <th>INVC</th>\n",
       "      <th>INVT</th>\n",
       "      <th>GC</th>\n",
       "      <th>HINC</th>\n",
       "      <th>PSIZE</th>\n",
       "      <th>choice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>345</td>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>148</td>\n",
       "      <td>115</td>\n",
       "      <td>160</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>325</td>\n",
       "      <td>55</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>38</td>\n",
       "      <td>255</td>\n",
       "      <td>66</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>300</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "      <td>305</td>\n",
       "      <td>51</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>305</td>\n",
       "      <td>75</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>305</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>465</td>\n",
       "      <td>116</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90</td>\n",
       "      <td>142</td>\n",
       "      <td>105</td>\n",
       "      <td>153</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TTME  INVC  INVT   GC  HINC  PSIZE   choice\n",
       "0    40    20   345   57    20       1       2\n",
       "1    45   148   115  160    45       1       1\n",
       "2    20    19   325   55    26       1       2\n",
       "3    15    38   255   66    26       1       2\n",
       "4    20    21   300   54     6       1       2\n",
       "5    45    18   305   51    20       1       2\n",
       "6    10    28   305   75    72       2       2\n",
       "7    20    21   305   54     6       1       2\n",
       "8    45    45   465  116    10       2       2\n",
       "9    90   142   105  153    50       1       1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_cleaned=df3_cleaned.drop(\"MODE\",axis=1)\n",
    "df3_cleaned=df3_cleaned.reset_index()\n",
    "df3_cleaned=df3_cleaned.drop(\"index\",axis=1)\n",
    "\n",
    "df3_cleaned.to_csv('travel_mode_choice.csv',index=False)\n",
    "df3_cleaned.iloc[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8f097",
   "metadata": {},
   "source": [
    "Now this data only includes 151 rows and every row represents for a single passenge. \"choice\" is the target variable and it contains three different categories: \"1\" for \"air\", \"2\" for \"train\" and \"3\" for \"bus\". All the X features are numeric variables. This cleaned data is ideal for any classification method now.\n",
    "\n",
    "You can find this cleaned data here:\\\n",
    "https://github.com/anly501/anly-501-project-WilliamChuFCB/tree/main/data/cleaned_data\n",
    "\n",
    "The name of this file is \"travel_mode_choice.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
