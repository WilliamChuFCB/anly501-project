[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Data Cleaning"
  },
  {
    "objectID": "index.html#data-cleaning-for-text-data",
    "href": "index.html#data-cleaning-for-text-data",
    "title": "Data Cleaning",
    "section": "Data cleaning for text data",
    "text": "Data cleaning for text data\nThe data used in this part is text data from Twitter API. You can find the raw data here:\nhttps://github.com/anly501/anly-501-project-WilliamChuFCB/tree/main/data/raw_data\nThe data mentioned above is named ‚Äútweet_freeway.csv‚Äù and ‚Äútweet_metro.csv‚Äù respectively.\nFirst import necessary packages and read the raw datasets\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nimport string \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# GET STOPWORDS\nfrom nltk.corpus import stopwords\nstop_words=nltk.corpus.stopwords.words('english')\n\n# INITALIZE STEMMER+LEMITZIZER+SIA\nsia = SentimentIntensityAnalyzer()\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\ndf1=pd.read_csv('tweet_freeway.csv') \ndf2=pd.read_csv('tweet_metro.csv') \n\nLet‚Äôs take a quick look at the raw data:\n\nlist(df1.iloc[10:19,0])\n\n[\"@RealDeniseWelch Wake up Ms holier than thou.  Harry is cashing in off Diana &amp; the only time Migraine doesn't lie is when she's asleep.\",\n 'I hate the 5 freeway so much',\n 'Westbound 91 Freeway closed over weekend https://t.co/ynRCJmjh72 https://t.co/Exo4XAIqO1',\n 'MULTI FR24 CIRCLING ALERT : At time Sat Dec 10 00:09:05 2022 #N2824Y was likely to be circling at FL34 1nm from LG‚Ä¶ https://t.co/KhDXGLDd3x',\n 'Freeway Fallguy #Juegos #Autos https://t.co/iNBoLT0rmp',\n 'if no control ever comes on when i‚Äôm driving i‚Äôll swerve off the freeway',\n '@TideorDieChick These niggas had a blockade set up on the freeway and did donuts!',\n 'I‚Äôm stuck in the freeway and I gotta go home and pack üò©',\n 'Freeway tells me ‚Äúdrive hammered, get nailed.‚Äù Uh oh']\n\n\n\nlist(df2.iloc[10:19,0])\n\n['@My_Metro The one next due (which is meant to be going to the coast based on your last reply) is showing as airport‚Ä¶ https://t.co/aCxaxVWU7o',\n '@My_Metro Thanks!',\n '@socialepfo @PMOIndia @byadavbjp @Rameswar_Teli @LabourMinistry @mygovindia @MIB_India @PIB_India @AmritMahotsav In‚Ä¶ https://t.co/OQW2sDWKtx',\n 'If you ever get time do notice that the  frequency of metro towards Noida are less as compared to metro towards Dwa‚Ä¶ https://t.co/DwzX97RT5f',\n 'RT @drdpgoel: On board the Nagpur Metro, Hon‚Äôble prime minister Shri @narendramodi ji interacted with students, those from the start up sec‚Ä¶',\n '\"#MetroInDino will have 4 segments, each story going on to be linked with one another in the end. #AnuragBasu had d‚Ä¶ https://t.co/UxvydjKtAv',\n '@jingyi25510683 Five players banned from World Snooker Tour over match-fixing allegations https://t.co/KBIIthv8Mm via @Metro_Sport',\n \"@Olegrio75499074 The Metro Police are a bunch of üåà snowflakes, they don't charge, and the attackers know it...\",\n 'Kaiser Permanente supports metro Atlanta communities to end homelessness(Sponsored Content by @KPGeorgia) https://t.co/FM8AVGJR25']\n\n\nIt is obvious that this raw data is difficult for doing further analysis due to unwanted characters and inconsistent format. Thus, we need to use a series of data cleaning method to polish these datasets.\n\ndef clean_string(text):\n    #FILTER OUT UNWANTED CHAR\n    new_text=\"\"\n    keep=\" abcdefghijklmnopqrstuvwxyz0123456789\"\n    for character in text:\n        if character.lower() in keep:\n            new_text+=character.lower()\n        else: \n            new_text+=\" \"\n    text=new_text\n\n    #FILTER OUT UNWANTED WORDS\n    new_text=\"\"\n    for word in nltk.tokenize.word_tokenize(text):\n        if word not in nltk.corpus.stopwords.words('english'):\n            #lemmatize \n            tmp=lemmatizer.lemmatize(word)\n            word=tmp\n            if len(word)>1:\n                if word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n                    #remove the last space\n                    new_text=new_text[0:-1]+word+\" \"\n                else: #add a space\n                    new_text+=word.lower()+\" \"\n    text=new_text.strip()\n    return text\n\n\nsh_free_list=list(df1.iloc[:,0])\nsh_mtr_list=list(df2.iloc[:,0])\nfor i in range(len(sh_free_list)):\n    sh_free_list[i]=clean_string(sh_free_list[i])\nfor i in range(len(sh_mtr_list)):\n    sh_mtr_list[i]=clean_string(sh_mtr_list[i])\n\nAfter the process above, we have filtered out unwanted characters, spaces and stopwords. Have a look at this data now:\n\nsh_free_list[10:19]\n\n['realdenisewelch wake holier thou harry cashing diana amp time migraine lie asleep',\n 'hate freeway much',\n 'westbound 91 freeway closed weekend http co ynrcjmjh72 http co exo4xaiqo1',\n 'multi fr24 circling alert time sat dec 10 00 09 05 2022 n2824y likely circling fl34 1nm lg http co khdxgldd3x',\n 'freeway fallguy juegos auto http co inbolt0rmp',\n 'control ever come driving swerve freeway',\n 'tideordiechick nigga blockade set freeway donut',\n 'stuck freeway got ta go home pack',\n 'freeway tell drive hammered get nailed uh oh']\n\n\nWe can see that the data is much cleaner now. We only keep English characters and numbers, and we also remove those words without actual meaning, like preposition. Perhaps you will find this data more difficult to understand, but it is absolutely much easier for the computer to comprehend.\nNext, we arrange labels to the data and combine tweets about freeway and metro into one dataset. This is important for further analysis, like some supervised classification algorithms.\n\ntmp1=[]\nfor i in range(0,len(sh_free_list)):\n    tmp1.append([sh_free_list[i],\"freeway\"])\ndf1=pd.DataFrame(tmp1)\ndf1=df1.rename(columns={0: \"text\", 1: \"label\"})\n\ntmp2=[]\nfor i in range(0,len(sh_mtr_list)):\n    tmp2.append([sh_mtr_list[i],\"metro\"])\ndf2=pd.DataFrame(tmp2)\ndf2=df2.rename(columns={0: \"text\", 1: \"label\"})\n\ndf=pd.concat([df1,df2])\nprint(df.head())\nprint(df.tail())\ndf.to_csv('free_mtr_text_data.csv',index=False)\n\n                                                text    label\n0  boenau rfsdfhsfbhwsfgb least ca expressway mig...  freeway\n1  zacakamadu sudden urge go san jose talk cultur...  freeway\n2        alinaaaziz yes girl freeway system wack lol  freeway\n3  multi adsbx circling alert time sat dec 10 18 ...  freeway\n4  ghettosmosh almost crashed fucking whip freewa...  freeway\n                                                  text  label\n876  mirazi8 enquiry case found mentioned address t...  metro\n877  drishtisharma02 even highlighted action taken ...  metro\n878  rt vjsubhash01 bos return chennai metro carry ...  metro\n879  rt rweingarten yesterday new mexico family amp...  metro\n880  stephedger one train departed next train due l...  metro\n\n\nThe data is ideal for further analysis now. You can access to this cleaned data here:\nhttps://github.com/anly501/anly-501-project-WilliamChuFCB/tree/main/data/cleaned_data\nThe name of this dataset is ‚Äúfree_mtr_text_data.csv‚Äù\nIn order to use algorithms to do classification on this dataset, we still need some steps to make the data more comprehensible for the computer. First, convert the string labels to integer labels using 0 and 1.\n\n#CONVERT FROM STRING LABELS TO INTEGERS \nlabels=[]\ny1=[]\nfor label in df[\"label\"]:\n    if label not in labels:\n        labels.append(label)\n        print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            y1.append(i)\ny1=np.array(y1)\n\n# CONVERT DF TO LIST OF STRINGS \ntextdata=df[\"text\"].to_list()\n\nprint(\"number of text chunks = \",len(textdata))\nprint(\"--------------------\")\nprint(\"example of the data:\")\nprint(textdata[0:3])\n\nindex = 0 : label = freeway\nindex = 1 : label = metro\nnumber of text chunks =  3465\n--------------------\nexample of the data:\n['boenau rfsdfhsfbhwsfgb least ca expressway might limited access still grade inte http co dibmsr9b5b', 'zacakamadu sudden urge go san jose talk cultural impact song unfortunately san jo http co nujnntbhx0', 'alinaaaziz yes girl freeway system wack lol']\n\n\nThen, vectorize the data and transform it into onehot matrix.\n\n# INITIALIZE COUNT VECTORIZER\nvectorizer=CountVectorizer(min_df=5)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \nvec=vectorizer.fit_transform(textdata)   \ndense=np.array(vec.todense())\n\n#CONVERT TO ONE-HOT VECTORS\nmaxs=np.max(dense,axis=0)\nonehot=np.ceil(dense/maxs)\n\n# DOUBLE CHECK \nprint(\"shape of onehot matrix:\")\nprint(onehot.shape)\n\nshape of onehot matrix:\n(3465, 1332)\n\n\n\nonehot\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [1., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\nWe can see that onehot matrix is absolutely a sparse matrix, where most values in this matrix is 0. Every row of this matrix represents a single text chunk while every column repersents a word. ‚Äú1‚Äù means the word represented by the column exists in the text chunk represented by the row. By contrast, ‚Äú0‚Äù means the word does not exist in the text chunk."
  },
  {
    "objectID": "index.html#data-cleaning-for-record-data",
    "href": "index.html#data-cleaning-for-record-data",
    "title": "Data Cleaning",
    "section": "Data cleaning for record data",
    "text": "Data cleaning for record data\nThis data comes from statsmodels.org. It contains information of passengers‚Äô different choice of travel mode.\nThe meanings of variables are as follows:\nTTME - terminal waiting time\nINVC - in vehicle cost for all stages\nINVT - travel time (in-vehicle time) for all stages\nGC - generalized cost measure:invc+(invt*value of travel time savings)\nHINC - household income\nPSIZE - traveling group size\nAs for the target variable ‚ÄúMODE‚Äù, it is relatively complex. Every four rows represent for one passenger and each row of these four represents ‚Äúair‚Äù, ‚Äútrain‚Äù, ‚Äúbus‚Äù, ‚Äúcar‚Äù respectively from top to bottom. The ‚ÄúMODE‚Äù variable is similar to an one-hot variable. When ‚ÄúMODE‚Äù equals to 1, it means this passenger choose the travel mode represented by the row where this 1 is located in.\nThe target of cleaning this data is to make it more comprehensible and easy for further analysis.\nYou can find the raw data here:\nhttps://github.com/anly501/anly-501-project-WilliamChuFCB/tree/main/data/raw_data\nThe name of this file is ‚Äútravel.csv‚Äù.\nRead from csv file and have a look at the raw data:\n\ndf3=pd.read_csv('travel.csv')\ndf3.iloc[:10,:]\n\n\n\n\n\n  \n    \n      \n      MODE\n      TTME\n      INVC\n      INVT\n      GC\n      HINC\n      PSIZE\n    \n  \n  \n    \n      0\n      0\n      69\n      59\n      100\n      70\n      35\n      1\n    \n    \n      1\n      0\n      34\n      31\n      372\n      71\n      35\n      1\n    \n    \n      2\n      0\n      35\n      25\n      417\n      70\n      35\n      1\n    \n    \n      3\n      1\n      0\n      10\n      180\n      30\n      35\n      1\n    \n    \n      4\n      0\n      64\n      58\n      68\n      68\n      30\n      2\n    \n    \n      5\n      0\n      44\n      31\n      354\n      84\n      30\n      2\n    \n    \n      6\n      0\n      53\n      25\n      399\n      85\n      30\n      2\n    \n    \n      7\n      1\n      0\n      11\n      255\n      50\n      30\n      2\n    \n    \n      8\n      0\n      69\n      115\n      125\n      129\n      40\n      1\n    \n    \n      9\n      0\n      34\n      98\n      892\n      195\n      40\n      1\n    \n  \n\n\n\n\nFirst, I notice that the data rows represent for ‚Äúcar‚Äù is quite different from other rows. For example, the ‚ÄúTTME‚Äù variable is always 0 for ‚Äúcar‚Äù and it is greater than 0 for others. This might lead to data skew, so I choose to remove all the rows related to ‚Äúcar‚Äù. The remained rows contain three categories: ‚Äúair‚Äù, ‚Äúbus‚Äù and ‚Äútrain‚Äù. All of these three are public transportation and it makes sense to do analysis among these three travel mode. Then, transform the onehot variable ‚ÄúMODE‚Äù into a typical class label ‚Äúchoice‚Äù.\n\ndf3[\"choice\"]=[1,2,3,4]*210\n\ndf3_cleaned=df3.loc[df3[\"MODE\"]==1,]\ndf3_cleaned=df3_cleaned.loc[df3[\"choice\"]!=4,]\n\nRemove the old variable ‚ÄúMODE‚Äù.\n\ndf3_cleaned=df3_cleaned.drop(\"MODE\",axis=1)\ndf3_cleaned=df3_cleaned.reset_index()\ndf3_cleaned=df3_cleaned.drop(\"index\",axis=1)\n\ndf3_cleaned.to_csv('travel_mode_choice.csv',index=False)\ndf3_cleaned.iloc[:10,:]\n\n\n\n\n\n  \n    \n      \n      TTME\n      INVC\n      INVT\n      GC\n      HINC\n      PSIZE\n      choice\n    \n  \n  \n    \n      0\n      40\n      20\n      345\n      57\n      20\n      1\n      2\n    \n    \n      1\n      45\n      148\n      115\n      160\n      45\n      1\n      1\n    \n    \n      2\n      20\n      19\n      325\n      55\n      26\n      1\n      2\n    \n    \n      3\n      15\n      38\n      255\n      66\n      26\n      1\n      2\n    \n    \n      4\n      20\n      21\n      300\n      54\n      6\n      1\n      2\n    \n    \n      5\n      45\n      18\n      305\n      51\n      20\n      1\n      2\n    \n    \n      6\n      10\n      28\n      305\n      75\n      72\n      2\n      2\n    \n    \n      7\n      20\n      21\n      305\n      54\n      6\n      1\n      2\n    \n    \n      8\n      45\n      45\n      465\n      116\n      10\n      2\n      2\n    \n    \n      9\n      90\n      142\n      105\n      153\n      50\n      1\n      1\n    \n  \n\n\n\n\nNow this data only includes 151 rows and every row represents for a single passenge. ‚Äúchoice‚Äù is the target variable and it contains three different categories: ‚Äú1‚Äù for ‚Äúair‚Äù, ‚Äú2‚Äù for ‚Äútrain‚Äù and ‚Äú3‚Äù for ‚Äúbus‚Äù. All the X features are numeric variables. This cleaned data is ideal for any classification method now.\nYou can find this cleaned data here:\nhttps://github.com/anly501/anly-501-project-WilliamChuFCB/tree/main/data/cleaned_data\nThe name of this file is ‚Äútravel_mode_choice.csv‚Äù"
  }
]