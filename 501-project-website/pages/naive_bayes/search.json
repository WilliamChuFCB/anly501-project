[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes for Text Data and Record Data"
  },
  {
    "objectID": "index.html#introduction-to-naive-bayes",
    "href": "index.html#introduction-to-naive-bayes",
    "title": "Naive Bayes",
    "section": "Introduction to Naive Bayes",
    "text": "Introduction to Naive Bayes\nNaive Bayesian algorithm is a classification algorithm designed based on conditional probability. Its underlying logic is Bayesian posterior probability, so it becomes a Bayesian classifier. The mathematical form of the problem can be written as:\n\\[\nP(B|A) = \\frac{P(A \\cap B)}{P(A)}\n\\]\nThe parameters in the formula will be defined as below.\nWe define Y as the classification of sample points. Here, we assume that the value of Y is discrete, and X represents the characteristics of the sample. In general, we can often know the proportion of certain characteristics in a certain category. So our ultimate goal is to get the final value of , which means the probability that the sample point is of classification Y given the character X. By analyzing the distribution of the population, we can directly use the proportion of different values of X in the population to approximate the overall and use the proportion of different X in a given category Y to represent the . The purpose of the algorithm is to calculate the conditional probabilities of a given X in different kinds by iteration, and take the maximum conditional probability as the classification of the sample point.\nTo be noticed, the algorithm requires the data set to be as large as possible to ensure the training data can represent the population in general case."
  },
  {
    "objectID": "index.html#preparations",
    "href": "index.html#preparations",
    "title": "Naive Bayes",
    "section": "Preparations",
    "text": "Preparations\nFirst import relevant packages for this task.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport wikipedia\nimport nltk\nimport string \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\n\n\nRead the “free_mtr_text_data.csv” file from the data cleaning part.\nYou can access to this data here:\nhttps://github.com/anly501/anly-501-project-WilliamChuFCB/tree/main/data/cleaned_data\nAfter read in the csv file, I also transform the string labels to integer labels for further modeling.\n\n\nCode\ndf=pd.read_csv('free_mtr_text_data.csv')  \nprint(\"shape of the dataset:\")\nprint(df.shape)\n\n#CONVERT FROM STRING LABELS TO INTEGERS \nlabels=[]\ny1=[]\nfor label in df[\"label\"]:\n    if label not in labels:\n        labels.append(label)\n        print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            y1.append(i)\ny1=np.array(y1)\n\n# CONVERT DF TO LIST OF STRINGS \ntextdata=df[\"text\"].to_list()\n\nprint(\"number of text chunks = \",len(textdata))\nprint(\"-------------------\")\nprint(\"examples of the data:\")\nprint(textdata[0:3])\n\n\nshape of the dataset:\n(3465, 2)\nindex = 0 : label = freeway\nindex = 1 : label = metro\nnumber of text chunks =  3465\n-------------------\nexamples of the data:\n['boenau rfsdfhsfbhwsfgb least ca expressway might limited access still grade inte http co dibmsr9b5b', 'zacakamadu sudden urge go san jose talk cultural impact song unfortunately san jo http co nujnntbhx0', 'alinaaaziz yes girl freeway system wack lol']\n\n\nThen, vectorize the data and transform it into onehot matrix.\n\n\nCode\n# INITIALIZE COUNT VECTORIZER\nvectorizer=CountVectorizer(min_df=5)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \nvec=vectorizer.fit_transform(textdata)   \ndense=np.array(vec.todense())\n\n#CONVERT TO ONE-HOT VECTORS\nmaxs=np.max(dense,axis=0)\nonehot=np.ceil(dense/maxs)\n\n# DOUBLE CHECK \nprint(\"shape of onehot matrix:\")\nprint(onehot.shape)\n\n\nshape of onehot matrix:\n(3465, 1332)"
  },
  {
    "objectID": "index.html#basic-eda-for-the-text-data",
    "href": "index.html#basic-eda-for-the-text-data",
    "title": "Naive Bayes",
    "section": "Basic EDA for the text data",
    "text": "Basic EDA for the text data\nCreate a heat map to visualize the distance matrix:\n\n\nCode\nnum_rows_keep=250\nindex=np.sort(np.random.choice(onehot.shape[0], num_rows_keep, replace=False))\ntmp1=onehot[index, :]\n\n#COMPUTE DISTANCE MATRIX\ndij=[]\n\n#LOOP OVER ROWS\nfor i in range(0,tmp1.shape[0]):\n    tmp2=[]\n    #LOOP OVER ROWS\n    for j in range(0,tmp1.shape[0]):\n\n        #EXTRACT VECTORS\n        vi=tmp1[i,:]\n        vj=tmp1[j,:]\n\n        #COMPUTE DISTANCES\n        dist=np.dot(vi, vj)/(np.linalg.norm(vi)*np.linalg.norm(vj)) \n        \n        # BUILD DISTANCE MATRIX\n        if(i==j or np.max(vi) == 0 or np.max(vj)==0):\n            tmp2.append(0)\n        else:\n            tmp2.append(dist)\n    dij.append(tmp2)\n        \ndij=np.array(dij)\n\nimport seaborn as sns\nfig,axes = plt.subplots(1, 1, num=\"stars\",figsize=(10, 8))\nplot1=sns.heatmap(dij, annot=False)\nplot1.set_title(\"Heat Map of the Distance Matrix\", fontsize=18)\nprint(dij.shape)\nprint(dij)\n\n\n(250, 250)\n[[0.         0.16903085 0.11952286 ... 0.         0.         0.        ]\n [0.16903085 0.         0.30304576 ... 0.         0.28571429 0.26726124]\n [0.11952286 0.30304576 0.         ... 0.         0.20203051 0.18898224]\n ...\n [0.         0.         0.         ... 0.         0.18898224 0.1767767 ]\n [0.         0.28571429 0.20203051 ... 0.18898224 0.         0.40089186]\n [0.         0.26726124 0.18898224 ... 0.1767767  0.40089186 0.        ]]\n\n\n\n\n\nThis is a heatmap of distance matrix of sentence vectors for a subset of the data.\nNext, perform PCA on this data and visualize the distribution of the data on first two and three principle components respectively using 2-D and 3-D scatter plot. Also, draw a pairplot for first ten principle components.\n\n\nCode\nfrom sklearn.decomposition import PCA\n\n# COMPUTE PCA WITH 10 COMPONENTS\npca = PCA(n_components=10)\npca.fit(onehot)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\n\n# GET PRINCIPLE COMPONENT PROJECTIONS \nprincipal_components = pca.fit_transform(onehot)\ndf2 = pd.DataFrame(data = principal_components) \ndf3=pd.concat([df2,df['label']], axis=1)\n\n# FIRST TWO COMPONENTS\nsns.scatterplot(data=df2, x=0, y=1,hue=df[\"label\"]) \nplt.show()\n\n#3D PLOT\nax = plt.axes(projection='3d')\nax.scatter3D(df2[0], df2[1], df2[2], c=y1);\nplt.show()\n\n#PAIRPLOT\nplot2=sns.pairplot(data=df3,hue=\"label\") \n#plt.show()\n\n\n[0.07857278 0.05298973 0.02185908 0.01781894 0.01384257 0.01342373\n 0.01034756 0.00872622 0.00811426 0.00747515]\n[43.26549563 35.53051175 22.82031075 20.60376245 18.1599245  17.88307279\n 15.70090348 14.41845457 13.90369045 13.34490494]\n\n\n\n\n\n\n\n\n\n\n\nAccording to the visualizations above, we can see that there exists obvious difference in data distribution of two labels of data. I will further use Naive Bayes to figure out if I can create a classification model with high accuracy based on this data."
  },
  {
    "objectID": "index.html#naive-bayes-for-text-data",
    "href": "index.html#naive-bayes-for-text-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes for text data",
    "text": "Naive Bayes for text data\nIn order to establish the Naive Bayes model, we need to split the data into training and testing sets.\n\n\nCode\n#split to train data and test data\n\nX=onehot\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y1, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\n\nx_train.shape       : (2772, 1332)\ny_train.shape       : (2772,)\nX_test.shape        : (693, 1332)\ny_test.shape        : (693,)\n\n\nTrain the model with Naive Bayes algorithm and predict on the testing set.\n\n\nCode\n# INITIALIZE MODEL \nmodel = MultinomialNB()\n\n# TRAIN MODEL \nmodel.fit(x_train,y_train)\n\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\nPrint the results of model prediction.\n\n\nCode\ndef report(y,ypred):\n    #ACCURACY COMPUTE \n    print(\"Accuracy:\",accuracy_score(y, ypred)*100)\n    print(\"Number of mislabeled points out of a total %d points = %d\"\n            % (y.shape[0], (y != ypred).sum()))\n\ndef print_model_summary():\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    print(\"ACCURACY CALCULATION\\n\")\n\n    print(\"TRAINING SET:\")\n    report(y_train,yp_train)\n\n    print(\"\\nTEST SET (UNTRAINED DATA):\")\n    report(y_test,yp_test)\n\n    print(\"\\nCHECK FIRST 20 PREDICTIONS\")\n    print(\"TRAINING SET:\")\n    print(y_train[0:20])\n    print(yp_train[0:20])\n    print(\"ERRORS:\",yp_train[0:20]-y_train[0:20])\n\n    print(\"\\nTEST SET (UNTRAINED DATA):\")\n    print(y_test[0:20])\n    print(yp_test[0:20])\n    print(\"ERRORS:\",yp_test[0:20]-y_test[0:20])\n    \n\nprint_model_summary()\n\n\nACCURACY CALCULATION\n\nTRAINING SET:\nAccuracy: 96.64502164502164\nNumber of mislabeled points out of a total 2772 points = 93\n\nTEST SET (UNTRAINED DATA):\nAccuracy: 93.93939393939394\nNumber of mislabeled points out of a total 693 points = 42\n\nCHECK FIRST 20 PREDICTIONS\nTRAINING SET:\n[1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0]\n[0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0]\nERRORS: [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n\nTEST SET (UNTRAINED DATA):\n[1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1]\n[1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1]\nERRORS: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\n\nWe can see that the prediction accuracy on the test dataset is 93.9%, which is very high. This means the model perform quite well on this dataset.\nI further measure the prediction result using confusion matrix.\n\n\nCode\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, yp_test.round()))\nprint(\"-----------------------\")\nprint(classification_report(y_test, yp_test.round()))\n\n\nConfusion Matrix:\n[[498  21]\n [ 21 153]]\n-----------------------\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96       519\n           1       0.88      0.88      0.88       174\n\n    accuracy                           0.94       693\n   macro avg       0.92      0.92      0.92       693\nweighted avg       0.94      0.94      0.94       693\n\n\n\n\n\nCode\nconfu=confusion_matrix(y_test, yp_test.round())\nfig,axes = plt.subplots(1, 1, num=\"stars\",figsize=(10, 8))\nplot3=sns.heatmap(confu)\nplot3.set_title(\"Heat Map of the Confusion Matrix\", fontsize=18)\nplt.savefig(\"confusion matrix py.png\")\n\n\n\n\n\nThis is a heat map of confusion matrix. The grids on top right-hand corner and left lower corner is black, which indicates that this model make few mistakes on the testing dataset."
  },
  {
    "objectID": "index.html#basic-eda-for-record-data",
    "href": "index.html#basic-eda-for-record-data",
    "title": "Naive Bayes",
    "section": "Basic EDA for record data",
    "text": "Basic EDA for record data\nIn this part, I will use record data to train the Naive Bayes model. First read the csv file “travel_mode_choice.csv”\nYou can find this cleaned data here:\nhttps://github.com/anly501/anly-501-project-WilliamChuFCB/tree/main/data/cleaned_data\n\n\nCode\ndf2=pd.read_csv(\"travel_mode_choice.csv\")\nX=np.array(df2)\ndf2.iloc[:10,:]\n\n\n\n\n\n\n  \n    \n      \n      TTME\n      INVC\n      INVT\n      GC\n      HINC\n      PSIZE\n      choice\n    \n  \n  \n    \n      0\n      40\n      20\n      345\n      57\n      20\n      1\n      2\n    \n    \n      1\n      45\n      148\n      115\n      160\n      45\n      1\n      1\n    \n    \n      2\n      20\n      19\n      325\n      55\n      26\n      1\n      2\n    \n    \n      3\n      15\n      38\n      255\n      66\n      26\n      1\n      2\n    \n    \n      4\n      20\n      21\n      300\n      54\n      6\n      1\n      2\n    \n    \n      5\n      45\n      18\n      305\n      51\n      20\n      1\n      2\n    \n    \n      6\n      10\n      28\n      305\n      75\n      72\n      2\n      2\n    \n    \n      7\n      20\n      21\n      305\n      54\n      6\n      1\n      2\n    \n    \n      8\n      45\n      45\n      465\n      116\n      10\n      2\n      2\n    \n    \n      9\n      90\n      142\n      105\n      153\n      50\n      1\n      1\n    \n  \n\n\n\n\nDraw a heat map of correlation matrix of this data:\n\n\nCode\nprint(df2.corr())\nfig,axes = plt.subplots(1, 1, num=\"stars\",figsize=(14, 12))\naxes = sns.heatmap(df2.corr(), vmin=-1, vmax=1,cmap=\"vlag\")\naxes.set_title(\"Heat Map of the Correlation Matrix\", fontsize=18)\n#plt.show()\n\n\n            TTME      INVC      INVT        GC      HINC    PSIZE     choice\nTTME    1.000000  0.464165 -0.152421  0.309941  0.142177  0.070189 -0.384252\nINVC    0.464165  1.000000 -0.429092  0.550372  0.362047 -0.040891 -0.693269\nINVT   -0.152421 -0.429092  1.000000  0.484061 -0.241788 -0.045246  0.678051\nGC      0.309941  0.550372  0.484061  1.000000  0.131839  0.079697 -0.057761\nHINC    0.142177  0.362047 -0.241788  0.131839  1.000000 -0.017023 -0.294000\nPSIZE   0.070189 -0.040891 -0.045246  0.079697 -0.017023  1.000000 -0.078319\nchoice -0.384252 -0.693269  0.678051 -0.057761 -0.294000 -0.078319  1.000000\n\n\nText(0.5, 1.0, 'Heat Map of the Correlation Matrix')"
  },
  {
    "objectID": "index.html#naive-bayes-for-record-data",
    "href": "index.html#naive-bayes-for-record-data",
    "title": "Naive Bayes",
    "section": "Naive Bayes for record data",
    "text": "Naive Bayes for record data\nSplit the data into training and testing sets at 0.2 test ratio:\n\n\nCode\n#split to train data and test data\n\ny=df2.loc[:,\"choice\"]\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=0)\n# y_train=y_train.flatten()\n# y_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"X_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\n\nx_train.shape       : (120, 7)\ny_train.shape       : (120,)\nX_test.shape        : (31, 7)\ny_test.shape        : (31,)\n\n\nTrain the Naive Bayes model and predict on the testing data:\n\n\nCode\n# INITIALIZE MODEL \nmodel = MultinomialNB()\n\n# TRAIN MODEL \nmodel.fit(x_train,y_train)\n\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\nResult of the model prediction:\n\n\nCode\ndef report(y,ypred):\n    #ACCURACY COMPUTE \n    print(\"Accuracy:\",accuracy_score(y, ypred)*100)\n    print(\"Number of mislabeled points out of a total %d points = %d\"\n            % (y.shape[0], (y != ypred).sum()))\n\ndef print_model_summary():\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    print(\"ACCURACY CALCULATION\\n\")\n\n    print(\"TRAINING SET:\")\n    report(y_train,yp_train)\n\n    print(\"\\nTEST SET (UNTRAINED DATA):\")\n    report(y_test,yp_test)\n\n\nprint_model_summary()\n\n\nACCURACY CALCULATION\n\nTRAINING SET:\nAccuracy: 79.16666666666666\nNumber of mislabeled points out of a total 120 points = 25\n\nTEST SET (UNTRAINED DATA):\nAccuracy: 74.19354838709677\nNumber of mislabeled points out of a total 31 points = 8\n\n\nWe can see that the accuracy on the test data is 74.2%. Since this data contains three different categories in the target variable, this accuracy is acceptable.\nUtilize confusion matrix to further measure the performance of this model:\n\n\nCode\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, yp_test.round()))\nprint(\"-----------------------\")\nprint(classification_report(y_test, yp_test.round()))\n\n\nConfusion Matrix:\n[[ 8  0  0]\n [ 0 10  4]\n [ 0  4  5]]\n-----------------------\n              precision    recall  f1-score   support\n\n           1       1.00      1.00      1.00         8\n           2       0.71      0.71      0.71        14\n           3       0.56      0.56      0.56         9\n\n    accuracy                           0.74        31\n   macro avg       0.76      0.76      0.76        31\nweighted avg       0.74      0.74      0.74        31\n\n\n\n\n\nCode\nconfu=confusion_matrix(y_test, yp_test.round())\nfig,axes = plt.subplots(1, 1, num=\"stars\",figsize=(10, 8))\nplot3=sns.heatmap(confu)\nplot3.set_title(\"Heat Map of the Confusion Matrix\", fontsize=18)\nplt.savefig(\"confusion matrix py.png\")\n\n\n\n\n\nFrom this heat map of confusion matrix, we can see that most predictions fall on the diagonal, which means this model performs well."
  }
]