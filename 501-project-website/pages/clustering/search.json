[
  {
    "objectID": "clustering.html#introductions",
    "href": "clustering.html#introductions",
    "title": "Clustering for Record Data",
    "section": "Introductions",
    "text": "Introductions\n\nIntroduction to Clustering Algorithms\nThis page shows the practice of three different clustering algorithms: k-means, DBSCAN, and hierarchical clustering.\n1. K-means\nKmeans algorithm is the most commonly used algorithm in clustering algorithm, which divides the interval according to the distance of samples. The basic step of the algorithm is to manually select k points as the center point of the sample cluster, calculate the distance from all the sample points to the k center points, and then select the center point that can reach the shortest distance as the classification of new samples, so that each sample is marked once.\nThe next step is to update the center point. The update rule of the center point is to calculate the intermediate point of each category, then change the intermediate point into a new initial point, and re perform the above operations. In this way, until the distance of each update is less than a threshold, the iteration is ended and the final classification result is determined.\nFinally, it should be noted that the kmeans algorithm needs to first determine a k value and the initial center point. One disadvantage of kmeans algorithm is that the classification result is a convex set, so there is no way to classify various shapes, and normalization must be done before kmeans algorithm.\n2. DBSCAN\nDensity-based spatial clustering of applications with noise, known as DBSCAN, is also one of the most common clustering algorithms and it is actually most cited in scientific literature. The core idea of DBSCAN is to cluster based on density. It can find the dense regions in the sample points and cluster according to the density. The algorithm mainly designs two parameters, one is the radius of the neighborhood, the other is the minimum number of points. In this algorithm, there are three types of points. The first is the core point, that is, there are more than the minimum number of points in the neighborhood of the point; The second is the boundary point, which is not the core point but located in the neighborhood of a point; The third point is noise point, as long as it is not inferior to the first two points, it is noise point.\nThe relationship between points can be divided into four types: the first is density congruence, that is, one point is in the neighborhood of another point; The second is that the density can reach, that is, if a core point can be connected with another point density through several core points with the same density, the density of this point and the core point can reach; If two points can be transmitted to each other in the neighborhood, they are said to be connected in density; The fourth is called density incoherent.\nThe idea of the algorithm is to first select a point as the initial point, and then use the point as the core point to find all the points that can reach the density of the point to form a cluster. Perform the above steps for all points to get a collection of clusters. While traversing, the cluster is classified according to the inclusion relationship of clusters.\n3. Hierarchical Clustering\nHierarchical clustering establishes a tree structure. Each layer of the tree is a binary tree structure. In the clustering tree, data points of different categories are the lowest layer of the tree, and the top layer of the tree is a cluster root node. There are two processes for creating a cluster tree: top-down and bottom-up.\nThe first type is the bottom-up decomposition method, known as Agglomerative. Each point is regarded as a cluster, the nearest two points are combined into a new cluster, and these operations are repeated for each cluster, until the number of classifications meets the requirements, or the distance between two clusters meets the requirements.\nThe second type is a top-down approach, known as Division. First, all the sample points are considered as a cluster. Next, the farthest point is split into two clusters. Next, the above operations are performed on each cluster until the distance between the two clusters reaches a certain distance or the number of classifications meets the requirements. This method consumes a lot of computing resources and requires more resources to meet the requirements.\n\n\nIntroduction to Model Selection Methods\n1. Elbow Method\nFor K-means clustering, we usually use inertia, sum of squared distances of samples to their closest cluster center, to measure how well a dataset is clustered by K-means. An ideal model is the one with both low inertia and a low number of clusters (k). Nevertheless, since inertia always decreases as the number k increases, this is actually a tradeoff. In order to find the optimal k, we can then find the elbow point in the inertia-k plot.\n2. Silhouette Method\nSilhouette is another method for us to select the optimal model. Silhouette analysis is utilized to find the separation distance between the resulting clusters. The silhouette coefficient has a range of [-1,1]. “1” indicates that the sample is far away from the neighboring clusters, while “0” indicates that the sample is very close to the boundary and negative values mean that the samples might be assigned to the wrong cluster."
  },
  {
    "objectID": "clustering.html#preparation-and-data-presentation",
    "href": "clustering.html#preparation-and-data-presentation",
    "title": "Clustering for Record Data",
    "section": "Preparation and Data Presentation",
    "text": "Preparation and Data Presentation\nimport python packages and read data from csv file\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport scipy as sp\n\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn\nfrom sklearn.cluster import KMeans\nimport sklearn.cluster\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n\ndf=pd.read_csv(\"travel_mode_choice.csv\")\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      TTME\n      INVC\n      INVT\n      GC\n      HINC\n      PSIZE\n      choice\n    \n  \n  \n    \n      0\n      40\n      20\n      345\n      57\n      20\n      1\n      2\n    \n    \n      1\n      45\n      148\n      115\n      160\n      45\n      1\n      1\n    \n    \n      2\n      20\n      19\n      325\n      55\n      26\n      1\n      2\n    \n    \n      3\n      15\n      38\n      255\n      66\n      26\n      1\n      2\n    \n    \n      4\n      20\n      21\n      300\n      54\n      6\n      1\n      2\n    \n  \n\n\n\n\n\ndf.shape\n\n(151, 7)\n\n\nThis data is aboat people’s travel mode choice and it contains 151 lines. There are 3 different travel mode choice in this data: “1” represents for air, “2” for train, and “3” for bus. As for X variables (features), there are 6 variables in total interpreted below:\nTTME - terminal waiting time\nINVC - in vehicle cost for all stages\nINVT - travel time (in-vehicle time) for all stages\nGC - generalized cost measure:invc+(invt*value of travel time savings)\nHINC - household income\nPSIZE - traveling group size\nThe goal of the clustering models is to figure out if the X features can be clustered into several groups. Also, we can compare the clustering results with the original true labels.\nFirst, let’s have a look at the distribution of the target variable: choice\n\ndf[\"choice\"].value_counts()\n\n2    63\n1    58\n3    30\nName: choice, dtype: int64\n\n\nSeperate the dataset into features and labels\n\nX=df.loc[:,df.columns!=\"choice\"]\ny=df.loc[:,\"choice\"]\n\nThen I plot the heat map of the correlation matrix to see the relationship among different variables.\n\nprint(df.corr())\nfig,axes = plt.subplots(1, 1, num=\"stars\",figsize=(14, 12))\naxes = sns.heatmap(df.corr(), vmin=-1, vmax=1,cmap=\"vlag\")\naxes.set_title(\"Heat Map of the Correlation Matrix\", fontsize=18)\n#plt.show()\nplt.savefig(\"correlation.png\",dpi=200)\n\n            TTME      INVC      INVT        GC      HINC    PSIZE     choice\nTTME    1.000000  0.464165 -0.152421  0.309941  0.142177  0.070189 -0.384252\nINVC    0.464165  1.000000 -0.429092  0.550372  0.362047 -0.040891 -0.693269\nINVT   -0.152421 -0.429092  1.000000  0.484061 -0.241788 -0.045246  0.678051\nGC      0.309941  0.550372  0.484061  1.000000  0.131839  0.079697 -0.057761\nHINC    0.142177  0.362047 -0.241788  0.131839  1.000000 -0.017023 -0.294000\nPSIZE   0.070189 -0.040891 -0.045246  0.079697 -0.017023  1.000000 -0.078319\nchoice -0.384252 -0.693269  0.678051 -0.057761 -0.294000 -0.078319  1.000000\n\n\n\n\n\nAlso, have a look at the pair plot\n\nsns.pairplot(df,hue=\"choice\")\nplt.show()\n\n\n\n\nFrom the visualizations above, variable PSIZE may not be a good feature for further clustering. Thus, remove this variable.\n\nX=df.iloc[:,0:5]"
  },
  {
    "objectID": "clustering.html#clustering",
    "href": "clustering.html#clustering",
    "title": "Clustering for Record Data",
    "section": "Clustering",
    "text": "Clustering\nFirst, normalize the X features using the StandardScaler function.\n\nscaler = StandardScaler()\nscaler.fit(X)\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScalerStandardScaler()\n\n\n\nnor_df=pd.DataFrame(scaler.transform(X)).set_axis(X.columns, axis=1)\n\nHave a quick look of data after normalization.\n\nnor_df.head()\n\n\n\n\n\n  \n    \n      \n      TTME\n      INVC\n      INVT\n      GC\n      HINC\n    \n  \n  \n    \n      0\n      0.231373\n      -1.038226\n      -0.163412\n      -1.240949\n      -0.588128\n    \n    \n      1\n      0.453056\n      2.300127\n      -0.944272\n      1.189842\n      0.684913\n    \n    \n      2\n      -0.655361\n      -1.064307\n      -0.231313\n      -1.288149\n      -0.282598\n    \n    \n      3\n      -0.877044\n      -0.568771\n      -0.468966\n      -1.028550\n      -0.282598\n    \n    \n      4\n      -0.655361\n      -1.012146\n      -0.316189\n      -1.311749\n      -1.301031\n    \n  \n\n\n\n\n\n1. K-means\nPerform K-means algorithm to do the clustering. In order to select the optimal hyperparameter, I use both silhouette method and elbow method. Silhouette check comes first.\n\ndef kmeans(X,nmax=20,i_plot=False):\n    X=np.ascontiguousarray(X) \n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        model = sklearn.cluster.KMeans(n_clusters=param).fit(X)\n        labels=model.predict(X)\n\n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue \n        \n        if(sil_scores[-1]>sil_max):\n            opt_param=param\n            sil_max=sil_scores[-1]\n            opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")  \n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\nkmeans_result=kmeans(nor_df,nmax=15, i_plot=True)\n\nOPTIMAL PARAMETER = 3\n\n\n\n\n\nFrom the result above, when cluster number k equals to 3, this model achieve the highest silhouette.\nThen visualize the clustering result. The X data actually contains 5 variables, but in order to visualize the result in a 2-D graph, I only choose the second and third features to plot.\n\ndef plot(X,color_vector):\n    fig, ax = plt.subplots()\n    ax.scatter(X.iloc[:,1], X.iloc[:,2],c=color_vector, cmap=\"viridis\") #, alpha=0.5) #, c=y\n    ax.set(xlabel='Feature-1 (x_2)', ylabel='Feature-2 (x_3)',\n    title='Cluster data')\n    ax.grid()\n    plt.show()\n\n\nplot(nor_df,kmeans_result)\n\n\n\n\nIt is clear to see that K-means has successfully seperated the data into three distinct groups.\nSecondly, use elbow method to double check the optimal model selection.\n\ncluster = range(1,11)\ncentroid = []\ninertia = []\nindex = []\n\nfor k in range(1,11):\n    kmeans = KMeans(n_clusters=k)\n    kmeans = kmeans.fit(nor_df)\n    index = kmeans.labels_\n    centroid = kmeans.cluster_centers_\n    inertia.append(kmeans.inertia_)\n\n\nfig, axes = plt.subplots(1, 1, figsize=(7, 5))\nsns.set_theme(style=\"whitegrid\",font_scale=1.5)\nsns.lineplot(ax=axes, x=cluster, y=inertia)\naxes.set_title('inertia')\nplt.xlabel('Cluster')\nplt.show()\n\n\n\n\nFrom this inertia-k plot, it is obvious that the elbow appears when k=3. This result is the same as silhouette method.\n\n\n2. DBSCAN\nPerform DBSCAN algorithm to do the clustering and use silhouette method to select the optimal hyperparameter.\n\ndef dbscan(X,nmax=20,i_plot=False):\n    X=np.ascontiguousarray(X) \n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        param=0.5*(param-1)\n        model = sklearn.cluster.DBSCAN(eps=param).fit(X)\n        labels=model.labels_\n            \n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue \n        \n        if(sil_scores[-1]>sil_max):\n            opt_param=param\n            sil_max=sil_scores[-1]\n            opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")  \n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\ndbscan_result = dbscan(nor_df,nmax=15,i_plot=True)\n\nOPTIMAL PARAMETER = 1.5\n\n\n\n\n\n\nplot(nor_df,dbscan_result)\n\n\n\n\nThe optimal hyperparameter of DBSCAN is 1.5. However, the plot above indicates that the DBSCAN method does not perform as well as K-means.\n\n\n3. Hierarchical (Agglomerative clustering)\nPerform Agglomerative clustering algorithm and use silhouette method to select the optimal hyperparameter.\n\ndef hierarchical(X,nmax=20,i_plot=False):\n    X=np.ascontiguousarray(X) \n    params=[]; sil_scores=[]\n    sil_max=-10\n    for param in range(2,nmax+1):\n        model = sklearn.cluster.AgglomerativeClustering(n_clusters=param).fit(X)\n        labels=model.labels_\n            \n        try:\n            sil_scores.append(sklearn.metrics.silhouette_score(X,labels))\n            params.append(param)\n        except:\n            continue \n\n        if(sil_scores[-1]>sil_max):\n            opt_param=param\n            sil_max=sil_scores[-1]\n            opt_labels=labels\n\n    print(\"OPTIMAL PARAMETER =\",opt_param)\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")  \n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels\n\n\nag_result=hierarchical(nor_df,nmax=15, i_plot=True)\n\nOPTIMAL PARAMETER = 3\n\n\n\n\n\nFrom the result above, when cluster number n equals to 3, this model achieve the highest silhouette.\n\nplot(nor_df,ag_result)\n\n\n\n\nSimilar to K-means, this plot shows that Agglomerative has successfully seperated X data into three groups.\nHave a look at the dendrogram of the agglomerative clustering result.\n\ndef plot_dendrogram(model, **kwargs):\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  \n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack(\n        [model.children_, model.distances_, counts]\n    ).astype(float)\n\n    dendrogram(linkage_matrix, **kwargs)\n\nmodel = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\nmodel = model.fit(nor_df)\nplt.title(\"Hierarchical Clustering Dendrogram\")\nplot_dendrogram(model, truncate_mode=\"level\", p=3)\nplt.xlabel(\"Number of points in node \")\nplt.show()\n\n\n\n\nThis also shows that clustering the data into three groups makes sense.\n\n\nFinal results\nSince we have already used silhouette method or elbow method to seek for the optimal parameter, now run the K-means and Agglomerative model with these parameters.\nK-means:\n\nmodel = sklearn.cluster.KMeans(n_clusters=3).fit(nor_df)\nlabels=model.predict(nor_df)\nplot(nor_df,labels)\n\n\n\n\nAgglomerative:\n\nmodel = sklearn.cluster.AgglomerativeClustering(n_clusters=3).fit(X)\nlabels=model.labels_\nplot(X,labels)"
  },
  {
    "objectID": "clustering.html#result-discussion",
    "href": "clustering.html#result-discussion",
    "title": "Clustering for Record Data",
    "section": "Result Discussion",
    "text": "Result Discussion\nFrom the Clustering part, we can see that K-means and Agglomerative perform well and successfully seperated the data into three groups. However, DBSCAN do not output as good result as other two methods. Possible reason is that the method based on density does not work well on this given dataset.\nThere is still a valuable question to discuss: whether the three clusters output by the algorithms coincide with the true labels?\nPlot the data with true labels using same features (x_2 and x_3):\n\nplot(nor_df,y)\n\n\n\n\nTo our surprise, the true data is not distributed as expected. Some data point seems to have “wrong” label.\nCheck the difference between true labels and predicted labels with confusion matrix.\n\ny=y.replace(1,0)\ny=y.replace(2,1)\ny=y.replace(3,2)\n\n\ndef confusion_plot(y_data,y_pred):\n    print(\"ACCURACY:\",sum(y_pred==y_data)/len(y_data))\n    con_matrix=confusion_matrix(y_data,y_pred)\n    print(con_matrix)\n    disp=ConfusionMatrixDisplay(confusion_matrix=con_matrix)\n    disp.plot()\n    plt.show()\n\n\nconfusion_plot(y,ag_result)\n\nACCURACY: 0.6225165562913907\n[[57  0  1]\n [ 5 27 31]\n [ 0 20 10]]\n\n\n\n\n\nThe comparation result above shows that using Agglomerative algorithm to predict the label reaches an accuracy of 0.62."
  },
  {
    "objectID": "clustering.html#conclusions",
    "href": "clustering.html#conclusions",
    "title": "Clustering for Record Data",
    "section": "Conclusions",
    "text": "Conclusions\nIn general, clustering on this travel mode dataset provides us with valuable information.\nFirst, we can see that the clustering methods can successfully seperate the data into three groups. However, the groups predicted by algorithms do not coincide with the true label perfectly.\nThe inconsistency of clustering result and the true labels can be interpreted as follows. First, the true labels represents for three different choice of travel mode: air, bus and train. These people’s travel data may also includes some hidden pattern other than travel mode choice. For instance, perhaps these three clusters acquired by algorithms represent for passengers with different types, partitioned by a feature that does not exist in the dataset, like age or occupation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Clustering",
    "section": "",
    "text": "1 Introduction\nIn the clustering tab, you will see whole workflows respectively using three different kinds of clustering algorithms - k-means, DBSCAN, and hierarchical clustering. Elbow Method and Silhouette Method are utilized for selecting the optimal model parameter."
  }
]