[
  {
    "objectID": "SVM_text_data.html#introduction-of-svm",
    "href": "SVM_text_data.html#introduction-of-svm",
    "title": "SVM for Text Data",
    "section": "Introduction of SVM",
    "text": "Introduction of SVM\nSupport vector machine is an algorithm set that can do regression and classification prediction. The output of the algorithm is a classification hyperplane. According to different input characteristics, different kernel functions, namely different hyperplane shapes, can be selected for classification.\nFor the classified model, SVM uses hard classification and soft classification. Hard classification is to train the classifier in a completely accurate way, which is reflected in that the constraint conditions are either black or white. Soft classification is another way by introducing a parameter to partly accept a few incorrectly classified points to tackle the cases that the points cannot be rigorously classified.\nFor the loss function, Support Vector Machine maximizes the distance of all points from the classification hyperplane, and uses dual algorithm and KKT algorithm to optimize, to calculate the parameters of the hyperplane. In the process of prediction, support vector machine uses symbolic functions to classify points according to their positions relative to the hyperplane."
  },
  {
    "objectID": "SVM_text_data.html#preparations",
    "href": "SVM_text_data.html#preparations",
    "title": "SVM for Text Data",
    "section": "Preparations",
    "text": "Preparations\nimport python packages and read data from csv file\n\nimport matplotlib.pyplot as plt\nimport nltk\nimport string \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\n\n\ndf=pd.read_csv('free_mtr_text_data.csv')  \nprint(df.shape)\n\n#CONVERT FROM STRING LABELS TO INTEGERS \nlabels=[]\ny1=[]\nfor label in df[\"label\"]:\n    if label not in labels:\n        labels.append(label)\n        print(\"index =\",len(labels)-1,\": label =\",label)\n    for i in range(0,len(labels)):\n        if(label==labels[i]):\n            y1.append(i)\ny1=np.array(y1)\n\n# CONVERT DF TO LIST OF STRINGS \ntextdata=df[\"text\"].to_list()\n\nprint(\"number of text chunks = \",len(textdata))\nprint(textdata[0:3])\n\n(4448, 2)\nindex = 0 : label = freeway\nindex = 1 : label = metro\nnumber of text chunks =  4448\n['woman accused killing mother wednesday morning reportedly called police confess officer found http co groxls9foq', 'people think taking train poor people train exciting part every commut http co 2uues0mrn4', 'natural history museum los angeles county 1975 2003 halona susanna harrison mercedes ca2wwr379 rosa park http co piorjpzjcd']\n\n\nThis is the text data originated from the twitter API. The label of this data has two catagories: “freeway” and “metro”. The goal of this SVM model is learn to recognize the right catagory (freeway or metro) from a single piece of text data. This data has already been cleaned.\nFirst, let’s see the distribution of two labels:\n\nprint(\"label=freeway(0): \",Counter(y1)[0])\nprint(\"label=metro(1): \",Counter(y1)[1])\n\nlabel=freeway(0):  2502\nlabel=metro(1):  1946\n\n\nconvert this text data to onehot matrix in order for machine learning models to input\n\n# INITIALIZE COUNT VECTORIZER\nvectorizer=CountVectorizer(min_df=5)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \nvec=vectorizer.fit_transform(textdata)   \ndense=np.array(vec.todense())\n\n#CONVERT TO ONE-HOT VECTORS\nmaxs=np.max(dense,axis=0)\nonehot=np.ceil(dense/maxs)\n\n# DOUBLE CHECK \nprint(onehot.shape,y1.shape)\n\n(4448, 1712) (4448,)"
  },
  {
    "objectID": "SVM_text_data.html#data-explorations",
    "href": "SVM_text_data.html#data-explorations",
    "title": "SVM for Text Data",
    "section": "Data explorations",
    "text": "Data explorations\nFollowing is heat map of distance matrix of first 250 variables\n\nnum_rows_keep=250\nindex=np.sort(np.random.choice(onehot.shape[0], num_rows_keep, replace=False))\ntmp1=onehot[index, :]\n\n#COMPUTE DISTANCE MATRIX\ndij=[]\n\n#LOOP OVER ROWS\nfor i in range(0,tmp1.shape[0]):\n    tmp2=[]\n    #LOOP OVER ROWS\n    for j in range(0,tmp1.shape[0]):\n\n        #EXTRACT VECTORS\n        vi=tmp1[i,:]\n        vj=tmp1[j,:]\n\n        #COMPUTE DISTANCES\n        dist=np.dot(vi, vj)/(np.linalg.norm(vi)*np.linalg.norm(vj)) \n        \n        # BUILD DISTANCE MATRIX\n        if(i==j or np.max(vi) == 0 or np.max(vj)==0):\n            tmp2.append(0)\n        else:\n            tmp2.append(dist)\n    dij.append(tmp2)\n        \ndij=np.array(dij)\n\nimport seaborn as sns\nfig,axes = plt.subplots(1, 1, num=\"stars\",figsize=(10, 8))\nplot1=sns.heatmap(dij, annot=False)\nplot1.set_title(\"Heat Map of the Distance Matrix\", fontsize=18)\nprint(dij.shape)\nprint(dij)\nplt.savefig(\"distance.png\")\n\n(250, 250)\n[[0.         0.         0.         ... 0.         0.22222222 0.23570226]\n [0.         0.         0.11952286 ... 0.         0.         0.        ]\n [0.         0.11952286 0.         ... 0.         0.         0.        ]\n ...\n [0.         0.         0.         ... 0.         0.         0.1118034 ]\n [0.22222222 0.         0.         ... 0.         0.         0.23570226]\n [0.23570226 0.         0.         ... 0.1118034  0.23570226 0.        ]]\n\n\n\n\n\nThen I use PCA and create 3D plot and pairplots to see how data in onehot matrix approximately distributed\n\nfrom sklearn.decomposition import PCA\n\n# COMPUTE PCA WITH 10 COMPONENTS\npca = PCA(n_components=10)\npca.fit(onehot)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\n\n# GET PRINCIPLE COMPONENT PROJECTIONS \nprincipal_components = pca.fit_transform(onehot)\ndf2 = pd.DataFrame(data = principal_components) \ndf3=pd.concat([df2,df['label']], axis=1)\n\n# FIRST TWO COMPONENTS\nsns.scatterplot(data=df2, x=0, y=1,hue=df[\"label\"]) \nplt.show()\n\n#3D PLOT\nax = plt.axes(projection='3d')\nax.scatter3D(df2[0], df2[1], df2[2], c=y1);\nplt.show()\n\n#PAIRPLOT\nplot2=sns.pairplot(data=df3,hue=\"label\") \n#plt.show()\nplt.savefig(\"pairplot.png\")\n\n[0.07272411 0.05176986 0.02206656 0.01963864 0.01734547 0.01327158\n 0.01097582 0.00931019 0.00816361 0.00716267]\n[48.8346509  41.20285902 26.90024032 25.37724952 23.84964287 20.86171974\n 18.97174436 17.47302548 16.36176375 15.32591002]\n\n\n\n\n\n\n\n\n\n\n\nFrom these plots, we can find that the data belongs to “freeway” and data belongs to “metro” can be roughly seperated by a hyperplane."
  },
  {
    "objectID": "SVM_text_data.html#data-modeling",
    "href": "SVM_text_data.html#data-modeling",
    "title": "SVM for Text Data",
    "section": "Data modeling",
    "text": "Data modeling\nNow I split the data into train data set and test data set where the ratio of test data is 0.2\n\n#split to train data and test data\n\nfrom sklearn.model_selection import train_test_split\nX=onehot\ntest_ratio=0.2\nx_train, x_test, y_train, y_test = train_test_split(X, y1, test_size=test_ratio, random_state=0)\ny_train=y_train.flatten()\ny_test=y_test.flatten()\n\nprint(\"x_train.shape        :\",x_train.shape)\nprint(\"y_train.shape        :\",y_train.shape)\n\nprint(\"x_test.shape     :\",x_test.shape)\nprint(\"y_test.shape     :\",y_test.shape)\n\nx_train.shape       : (3558, 1712)\ny_train.shape       : (3558,)\nx_test.shape        : (890, 1712)\ny_test.shape        : (890,)\n\n\nSince the onehot matrix is acctually a sparse matrix, the mean of each columns is close to 0. Thus, there’s no need to nomalize this data.\n\nnp.mean(x_train.std(axis=0))\n\n0.05568728162523648\n\n\nNow I use SVM model to fit the train data set and I choose “Linear” kernel.\n\nmodel = SVC(C=0.5, kernel='linear')\nmodel.fit(x_train, y_train)\n\nSVC(C=0.5, kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(C=0.5, kernel='linear')\n\n\n\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\n\n\n# Calculate the confusion matrix and classification report for the train and test data. \ncon_matrix_train = confusion_matrix(y_train,yp_train,labels=model.classes_)\nprint(\"train:\")\nprint(con_matrix_train)\nprint(\"----------------------------\")\ncon_matrix_test = confusion_matrix(y_test,yp_test,labels=model.classes_)\nprint(\"test:\")\nprint(con_matrix_test)\n\ntrain:\n[[1948   31]\n [   7 1572]]\n----------------------------\ntest:\n[[486  37]\n [ 25 342]]"
  },
  {
    "objectID": "SVM_text_data.html#model-tuning",
    "href": "SVM_text_data.html#model-tuning",
    "title": "SVM for Text Data",
    "section": "Model tuning",
    "text": "Model tuning\nFrom the results, we can see that SVM model perform well on this data. Next, I will try other different kernels to find whether the accuracy of SVM model can be further improved.\n\ndef svm_acc(x_train,y_train,x_test,y_test,kernel):\n    model = SVC(C=0.5, kernel=kernel)\n    model.fit(x_train, y_train)\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    train_report = classification_report(y_train,yp_train,output_dict = True)\n    test_report = classification_report(y_test,yp_test,output_dict = True)\n    train_report_df = pd.DataFrame.from_dict(train_report).T\n    test_report_df = pd.DataFrame.from_dict(test_report).T\n    train_acc=train_report_df.loc[\"accuracy\",\"precision\"]\n    test_acc=test_report_df.loc[\"accuracy\",\"precision\"]\n    return train_acc,test_acc\n\nI will test linear kernel, polynomial kernel, rbf kernel and sigmoid kernel successively as follows.\n\nkernel_list=[\"linear\",\"poly\",\"rbf\",\"sigmoid\"]\n\n\ntrain_accs=[]\ntest_accs=[]\nfor k in kernel_list:\n    train_acc,test_acc=svm_acc(x_train,y_train,x_test,y_test,k)\n    train_accs.append(train_acc)\n    test_accs.append(test_acc)\n\n\n# set width of bar\nbarWidth = 0.25\nfig = plt.subplots(figsize =(12, 8))\n \n# Set position of bar on X axis\nbr1 = np.arange(len(train_accs))\nbr2 = [x + barWidth for x in br1]\n \n# Make the plot\nplt.bar(br1, train_accs, color ='r', width = barWidth,\n        edgecolor ='grey', label ='train_data')\nplt.bar(br2, test_accs, color ='b', width = barWidth,\n        edgecolor ='grey', label ='test_data')\n \n# Adding Xticks\nplt.xlabel('Kernel', fontweight ='bold', fontsize = 15)\nplt.ylabel('Accuracy', fontweight ='bold', fontsize = 15)\nplt.xticks([r + barWidth for r in range(len(train_accs))],\n        ['linear', 'poly', 'rbf', 'sigmoid'],fontsize=15)\n \nplt.legend()\nplt.show()\n\n\n\n\nAccording to this graph above, we can see that linear kernel have best accuracy on both train data and test data. Therefore, I choose linear kernel as the final hyperparameter."
  },
  {
    "objectID": "SVM_text_data.html#baseline-comparison",
    "href": "SVM_text_data.html#baseline-comparison",
    "title": "SVM for Text Data",
    "section": "Baseline comparison",
    "text": "Baseline comparison\nAdditionally, let’s compare our model’s result with baseline model(random classifier):\n\ndef generate_label_data(class_labels, weights,N=10000):\n    #e.g. class_labels=[0,1]  weights=[0.2,0.8] (should sum to one)\n\n    y=random.choices(class_labels, weights = weights, k = N)\n    return y\n\n\ndef random_classifier(y_data):\n    ypred=[];\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\",Counter(ypred).values()) # counts the elements' frequency\n    print(\"probability of prediction:\",np.fromiter(Counter(ypred).values(), dtype=float)/len(y_data)) # counts the elements' frequency\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data, ypred))\n\ny=generate_label_data([0,1],[0.56,0.44],10000)\nrandom_classifier(y)\n\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([4972, 5028])\nprobability of prediction: [0.4972 0.5028]\naccuracy 0.507\npercision, recall, fscore, (array([0.57220434, 0.44252188]), array([0.50371813, 0.51125919]), array([0.53578154, 0.47441365]), array([5648, 4352]))\n\n\nThere’s no doubt that our SVM model(whose accuracy is greater than 0.9) have much better performance than random classifier(whose accuracy is close to 0.5)."
  },
  {
    "objectID": "SVM_text_data.html#final-result-and-conclusion",
    "href": "SVM_text_data.html#final-result-and-conclusion",
    "title": "SVM for Text Data",
    "section": "Final result and conclusion",
    "text": "Final result and conclusion\n\nmodel = SVC(C=0.5, kernel='linear')\nmodel.fit(x_train, y_train)\nyp_train = model.predict(x_train)\nyp_test = model.predict(x_test)\ncon_matrix_train = confusion_matrix(y_train,yp_train,labels=model.classes_)\ncon_matrix_test = confusion_matrix(y_test,yp_test,labels=model.classes_)\n\n\ntrain_report = classification_report(y_train,yp_train,output_dict = True)\ntest_report = classification_report(y_test,yp_test,output_dict = True)\n\n# Save the results in a data frame. \ntrain_report_df = pd.DataFrame.from_dict(train_report).T\ntest_report_df = pd.DataFrame.from_dict(test_report).T\n\nprint(\"train_report:\")\nprint(train_report_df)\nprint(\"test_report:\")\nprint(test_report_df)\n\ntrain_report:\n              precision    recall  f1-score     support\n0              0.996419  0.984336  0.990341  1979.00000\n1              0.980661  0.995567  0.988058  1579.00000\naccuracy       0.989320  0.989320  0.989320     0.98932\nmacro avg      0.988540  0.989951  0.989199  3558.00000\nweighted avg   0.989426  0.989320  0.989328  3558.00000\ntest_report:\n              precision    recall  f1-score     support\n0              0.951076  0.929254  0.940039  523.000000\n1              0.902375  0.931880  0.916890  367.000000\naccuracy       0.930337  0.930337  0.930337    0.930337\nmacro avg      0.926725  0.930567  0.928464  890.000000\nweighted avg   0.930994  0.930337  0.930493  890.000000\n\n\nplot the confusion matrix of test data set:\n\ndisp=ConfusionMatrixDisplay(confusion_matrix=con_matrix_test,display_labels=model.classes_)\ndisp.plot()\n\n<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ff72b32b430>\n\n\n\n\n\nAccording to model reports above, the accuracy on test data set is over 0.93, which indicates that this SVM model perform quite well and does not overfit too much. This also means that when we give a piece of new text data to this SVM model (note that this piece of data needs to follow the distribution of the data in used above), the SVM model can recognize whether it relates to “freeway” or “metro” with a really high accuracy.\nIn this data modeling process, I actually only need to consider a binary classification problem. In more complicated situation, sometimes we have to deal with those data that contains more than two catagories. For example, there might be classes like “car”, “bus”, “airplane” and “ship”. Under this circumstance, we need to be more careful optimizing and tuning the model."
  },
  {
    "objectID": "DT_record_data.html",
    "href": "DT_record_data.html",
    "title": "Decision Tree for Record Data",
    "section": "",
    "text": "Preparations\nimport python packages and read data from csv file\nThis data is aboat people’s travel mode choice and it contains 151 lines. There are 3 different travel mode choice in this data: “1” represents for air, “2” for train, and “3” for bus. As for X variables (features), there are 6 variables in total interpreted below:\nTTME - terminal waiting time\nINVC - in vehicle cost for all stages\nINVT - travel time (in-vehicle time) for all stages\nGC - generalized cost measure:invc+(invt*value of travel time savings)\nHINC - household income\nPSIZE - traveling group size\nThe goal of this decision tree model is to precisely predict people’s travel mode choice according to those X variables."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SVM and Decision Tree",
    "section": "",
    "text": "1 Introduction\nOn this page, you can find two tabs on the left side: SVM for text data, and Decision Tree for record data.\nIn these two parts, you will see whole workflows respectively using two different kinds of machine learning algorithms - support vector machines, and decision trees."
  },
  {
    "objectID": "DT_record_data.html#introduction-of-dt",
    "href": "DT_record_data.html#introduction-of-dt",
    "title": "Decision Tree for Record Data",
    "section": "Introduction of DT",
    "text": "Introduction of DT\nThe decision tree is an attribute structure. Each leaf node of the decision tree represents a feature of the attribute set. Each branch is classified based on the size or category of a feature. The final result, that is, the final leaf node is a label classification, label classification.\nDecision tree has several classification algorithms according to different classification methods. ID3 algorithm is a kind of decision tree, which is based on Occam’s razor principle, that is, to do more with as few things as possible. The goal of each layer classification is to maximize the information gain, and the classification criteria of each layer are designed for this goal. C4.5 decision tree is optimized based on ID3 decision tree. Replacing the node division criteria with the information gain rate can handle continuous values, missing values, and pruning operations.\nThe pruning of decision trees is to reduce the complexity of decision trees and improve the generalization performance of decision trees. The generalization ability of the model can be reflected from the performance of the verification set."
  },
  {
    "objectID": "DT_record_data.html#data-exploration",
    "href": "DT_record_data.html#data-exploration",
    "title": "Decision Tree for Record Data",
    "section": "Data exploration",
    "text": "Data exploration\nFirst, let’s have a look at the distribution of the target variable: choice\n\ndf[\"choice\"].value_counts()\n\n2    63\n1    58\n3    30\nName: choice, dtype: int64\n\n\n\nX=df.loc[:,df.columns!=\"choice\"]\nY=df.loc[:,\"choice\"]\n\nThen I plot the heat map of the correlation matrix to see the relationship among different variables.\n\nprint(df.corr())\nfig,axes = plt.subplots(1, 1, num=\"stars\",figsize=(14, 12))\naxes = sns.heatmap(df.corr(), vmin=-1, vmax=1,cmap=\"vlag\")\naxes.set_title(\"Heat Map of the Correlation Matrix\", fontsize=18)\n#plt.show()\nplt.savefig(\"correlation.png\",dpi=200)\n\n            TTME      INVC      INVT        GC      HINC    PSIZE     choice\nTTME    1.000000  0.464165 -0.152421  0.309941  0.142177  0.070189 -0.384252\nINVC    0.464165  1.000000 -0.429092  0.550372  0.362047 -0.040891 -0.693269\nINVT   -0.152421 -0.429092  1.000000  0.484061 -0.241788 -0.045246  0.678051\nGC      0.309941  0.550372  0.484061  1.000000  0.131839  0.079697 -0.057761\nHINC    0.142177  0.362047 -0.241788  0.131839  1.000000 -0.017023 -0.294000\nPSIZE   0.070189 -0.040891 -0.045246  0.079697 -0.017023  1.000000 -0.078319\nchoice -0.384252 -0.693269  0.678051 -0.057761 -0.294000 -0.078319  1.000000"
  },
  {
    "objectID": "DT_record_data.html#data-modeling",
    "href": "DT_record_data.html#data-modeling",
    "title": "Decision Tree for Record Data",
    "section": "Data modeling",
    "text": "Data modeling\nIn data modeling part, we need to split the data first. Ratio of test data set is 0.2.\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\nprint(type(X_train),X_train.shape)\nprint(type(Y_train),Y_train.shape)\nprint(type(X_test),X_test.shape)\nprint(type(Y_test),Y_test.shape)\n\n<class 'pandas.core.frame.DataFrame'> (120, 6)\n<class 'pandas.core.series.Series'> (120,)\n<class 'pandas.core.frame.DataFrame'> (31, 6)\n<class 'pandas.core.series.Series'> (31,)\n\n\nThen it’s time to use decision tree model to fit the train data. We first use default hyperparameters.\n\nmodel = tree.DecisionTreeClassifier()\nmodel = model.fit(X_train,Y_train)\nyp_train = model.predict(X_train)\nyp_test = model.predict(X_test)\n\n\ndef confusion_plot(y_data,y_pred):\n    print(\"ACCURACY:\",sum(y_pred==y_data)/len(y_data))\n    con_matrix=confusion_matrix(y_data,y_pred,labels=model.classes_)\n    print(con_matrix)\n    disp=ConfusionMatrixDisplay(confusion_matrix=con_matrix,display_labels=model.classes_)\n    disp.plot()\n    plt.show()\n\n\nprint(\"------TRAINING------\")\nconfusion_plot(Y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(Y_test,yp_test)\n\n------TRAINING------\nACCURACY: 1.0\n[[45  0  0]\n [ 0 51  0]\n [ 0  0 24]]\n\n\n\n\n\n------TEST------\nACCURACY: 0.7096774193548387\n[[10  2  1]\n [ 0 11  1]\n [ 0  5  1]]\n\n\n\n\n\nAccording to these two confusion matrix, we can see that the accuracy on train data reaches an incredible 1.0. Nonetheless, test accuracy is much lower, which means this decision tree model with default hyperparameter faces severe overfitting problem.\n\ndef plot_tree(model,X,Y):\n    fig=plt.figure(figsize=(25,20))\n    _=tree.plot_tree(model,\n                   feature_names=X_train.columns,\n                   class_names=[\"1\",\"2\",\"3\"],\n                   filled=True)\n    plt.show()\n    \nplot_tree(model,X_train,Y_train)\n\n\n\n\nThis graph above is a clear illustration of this decision tree model on train data. It is obvious this tree is so deep that it fits every single point in train data, and that’s why it can achieve a 100 percent accuracy (but this is actually useless)"
  },
  {
    "objectID": "DT_record_data.html#model-tuning",
    "href": "DT_record_data.html#model-tuning",
    "title": "Decision Tree for Record Data",
    "section": "Model tuning",
    "text": "Model tuning\nIn order to overcome the overfitting problem and search for an optimal model, I will consider two hyperparameter to tune: max depth of the tree, and splitting criterion. First we start with the former one.\n\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = tree.DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(X_train,Y_train)\n\n    yp_train=model.predict(X_train)\n    yp_test=model.predict(X_test)\n\n    test_results.append([num_layer,accuracy_score(Y_test, yp_test)])\n    train_results.append([num_layer,accuracy_score(Y_train, yp_train)])\n\n\nfig,axes=plt.subplots(1,1,figsize=(9,7))\nfs=16\naxes.plot(np.array(train_results)[:,0],np.array(train_results)[:,1],\"b-o\")\naxes.plot(np.array(test_results)[:,0],np.array(test_results)[:,1],\"r-o\")\naxes.set_xlabel(\"Number of Layers in Decision Tree (max_depth)\", fontsize=fs)\naxes.set_ylabel(\"ACCURACY (Y=0): Training (blue) and Test (red)\", fontsize=fs)\n\nplt.show()\n\n\n\n\nAccording to this plot, it is clear that when max depth is greater than 7, accuracy on train data keep enhancing but performance on test data does not improve anymore. Thus, I choose 7 as the final value of this hyperparameter.\nNext, I will successively test two splitting criterion: “gini”, and “entropy”, fixing max depth to be 7.\n\ncriterion_list=[\"gini\", \"entropy\"]\n\n\ntest_results=[]\ntrain_results=[]\n\nfor crit in criterion_list:\n    model = tree.DecisionTreeClassifier(criterion=crit,max_depth=7)\n    model = model.fit(X_train,Y_train)\n\n    yp_train=model.predict(X_train)\n    yp_test=model.predict(X_test)\n\n    test_results.append(accuracy_score(Y_test, yp_test))\n    train_results.append(accuracy_score(Y_train, yp_train))\n\n\n# set width of bar\nbarWidth = 0.25\nfig = plt.subplots(figsize =(12, 8))\n \n# Set position of bar on X axis\nbr1 = np.arange(len(train_results))\nbr2 = [x + barWidth for x in br1]\n \n# Make the plot\nplt.bar(br1, train_results, color ='r', width = barWidth,\n        edgecolor ='grey', label ='train_data')\nplt.bar(br2, test_results, color ='b', width = barWidth,\n        edgecolor ='grey', label ='test_data')\n \n# Adding Xticks\nplt.xlabel('criterion', fontweight ='bold', fontsize = 15)\nplt.ylabel('Accuracy', fontweight ='bold', fontsize = 15)\nplt.xticks([r + barWidth - 0.1 for r in range(len(train_results))],\n        ['gini','entropy'],fontsize=15)\n \nplt.legend()\nplt.show()\n\n\n\n\nFrom this comparison, we can notice that model with “gini” criterion perform better on train data while “entropy” model have higher accuracy on test data. Therefore, we finally choose “entropy” as the criterion."
  },
  {
    "objectID": "DT_record_data.html#baseline-comparison",
    "href": "DT_record_data.html#baseline-comparison",
    "title": "Decision Tree for Record Data",
    "section": "Baseline comparison",
    "text": "Baseline comparison\nAdditionally, let’s compare this decision tree model with baseline model(random classifier):\n\ndef generate_label_data(class_labels, weights,N=10000):\n    y=random.choices(class_labels, weights = weights, k = N)\n    return y\n\n\ndef random_classifier(y_data):\n    ypred=[];\n    max_label=np.max(y_data); #print(max_label)\n    for i in range(0,len(y_data)):\n        ypred.append(int(np.floor((max_label+1)*np.random.uniform(0,1))))\n\n    print(\"-----RANDOM CLASSIFIER-----\")\n    print(\"count of prediction:\",Counter(ypred).values()) # counts the elements' frequency\n    print(\"probability of prediction:\",np.fromiter(Counter(ypred).values(), dtype=float)/len(y_data)) # counts the elements' frequency\n    print(\"accuracy\",accuracy_score(y_data, ypred))\n    print(\"percision, recall, fscore,\",precision_recall_fscore_support(y_data, ypred))\n\ny=generate_label_data([0,1,2],[0.42,0.38,0.20],10000)\nrandom_classifier(y)\n\n-----RANDOM CLASSIFIER-----\ncount of prediction: dict_values([3432, 3312, 3256])\nprobability of prediction: [0.3432 0.3312 0.3256]\naccuracy 0.3305\npercision, recall, fscore, (array([0.43206522, 0.37407862, 0.19114219]), array([0.33426769, 0.31976897, 0.3434555 ]), array([0.37692612, 0.3447983 , 0.2456009 ]), array([4281, 3809, 1910]))\n\n\nThere’s no doubt that our decision tree model performs much better than baseline model (random classifier)."
  },
  {
    "objectID": "DT_record_data.html#final-result-and-conclusion",
    "href": "DT_record_data.html#final-result-and-conclusion",
    "title": "Decision Tree for Record Data",
    "section": "Final result and conclusion",
    "text": "Final result and conclusion\n\nmodel = tree.DecisionTreeClassifier(criterion=\"entropy\",max_depth=7)\nmodel = model.fit(X_train,Y_train)\n\nyp_train=model.predict(X_train)\nyp_test=model.predict(X_test)\n\n\nprint(\"------TRAINING------\")\nconfusion_plot(Y_train,yp_train)\nprint(\"------TEST------\")\nconfusion_plot(Y_test,yp_test)\n\nplot_tree(model,X_test,Y_test)\n\n------TRAINING------\nACCURACY: 0.9\n[[45  0  0]\n [ 0 41 10]\n [ 0  2 22]]\n\n\n\n\n\n------TEST------\nACCURACY: 0.8387096774193549\n[[12  0  1]\n [ 1 11  0]\n [ 0  3  3]]\n\n\n\n\n\n\n\n\nAccording to model results and confusion matrix above, the accuracy on test data is greater than 0.8, which means this decision tree model performs as an excellent classifier to recognize three different catagories of travel mode “air”, “train”, and “bus” according to relevant data.\nOne advantage of decision tree model is that the result of this tree model is interpretable. For instance, from the tree plot above we notice that in the root node, the model split this data by “whether total travel time \\(\\leq\\) 211 minutes”. It makes sense that taking the train or long distance bus cost much more time than airplane, that’s why we immediately get a pure leaf under this root node.\nHowever, this data set is relatively small, and therefore the decision tree model might not be robust and stable enough. In future research, it will be a possible way to first do some data augmentation."
  },
  {
    "objectID": "DT_record_data.html#preparations",
    "href": "DT_record_data.html#preparations",
    "title": "Decision Tree for Record Data",
    "section": "Preparations",
    "text": "Preparations\nimport python packages and read data from csv file\n\n#LOAD RELEVANT PACKAGES\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as  pd\nfrom sklearn import tree\nfrom IPython.display import Image\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport random\nfrom collections import Counter\nfrom sklearn.metrics import precision_recall_fscore_support\n\n\ndf=pd.read_csv(\"travel_mode_choice.csv\")\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      TTME\n      INVC\n      INVT\n      GC\n      HINC\n      PSIZE\n      choice\n    \n  \n  \n    \n      0\n      40\n      20\n      345\n      57\n      20\n      1\n      2\n    \n    \n      1\n      45\n      148\n      115\n      160\n      45\n      1\n      1\n    \n    \n      2\n      20\n      19\n      325\n      55\n      26\n      1\n      2\n    \n    \n      3\n      15\n      38\n      255\n      66\n      26\n      1\n      2\n    \n    \n      4\n      20\n      21\n      300\n      54\n      6\n      1\n      2\n    \n  \n\n\n\n\nThis data is aboat people’s travel mode choice and it contains 151 lines. There are 3 different travel mode choice in this data: “1” represents for air, “2” for train, and “3” for bus. As for X variables (features), there are 6 variables in total interpreted below:\n\nTTME - terminal waiting time\nINVC - in vehicle cost for all stages\nINVT - travel time (in-vehicle time) for all stages\nGC - generalized cost measure:invc+(invt*value of travel time savings)\nHINC - household income\nPSIZE - traveling group size\n\nThe goal of this decision tree model is to precisely predict people’s travel mode choice according to those X variables."
  }
]