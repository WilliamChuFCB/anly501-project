[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ARM and Networking",
    "section": "",
    "text": "1 ARM and Networking"
  },
  {
    "objectID": "index.html#introduction-to-arm",
    "href": "index.html#introduction-to-arm",
    "title": "ARM and Networking",
    "section": "1.1 Introduction to ARM",
    "text": "1.1 Introduction to ARM\nAssociation rule analysis was first proposed to obtain the potential association between different commodities in transactions. This technology uses the expression of set. First, we define item set as \\(I=\\{i_1,i_2,\\cdots,i_p\\}\\), where each \\(i_k\\) can be viewed as an item (commodity). Second, we define transaction set as \\(D=\\{T_1,T_2,\\cdots,T_d\\}\\) where each \\(T_k\\) is a subset of I. The aim of our analysis is to obtain certain rules from D.\nFor a single project set, we define support to measure the possibility of the occurrence of the project set. The definition of support is similar to Bayesian probability, which is:\n\\[support(X)=\\frac{\\vert \\{T_i|X \\subseteq T_i\\}\\vert}{\\vert D\\vert}\\]\nThe support here can be understood as the possibility of the occurrence of such a project portfolio (commodity portfolio) as X, and it can also be understood as the possibility of such a group of commodities as X becoming complementary products to each other. For rules like \\(X \\rightarrow Y\\), we can calculate the similar supporting value by \\(\\frac{\\vert \\{T_i|X \\cup Y \\subseteq T_i\\}\\vert}{\\vert D\\vert}\\). This can predict the probability of Y’s commodity combination, based on the condition that X’s combination has been found.\nIn the algorithm, we set the minimum confidence threshold to give the basic item set, which is to prevent invalid sets, such as single element sets. At the same time, for the second type of rules (transitive rules), we use the maximum support method to generate the most effective association rules. The specific construction method uses the FP tree construction method to generate frequent item sets from the bottom up, sort by decreasing support, and obtain the combination of maximum support that meets the minimum threshold."
  },
  {
    "objectID": "index.html#preparations",
    "href": "index.html#preparations",
    "title": "ARM and Networking",
    "section": "1.2 Preparations",
    "text": "1.2 Preparations\nIn This page, I will use cleaned text data to do the ARM and Networking modeling. The data can be find here:\nhttps://github.com/anly501/anly-501-project-WilliamChuFCB/tree/main/data/cleaned_data\nFirst import necessary packages:\n\nimport nltk\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom apyori import apriori\nimport networkx as nx \n\nDefine some global variables:\n\n#USER PARAM\ninput_path          =   'free_mtr_text_data.csv'\ncompute_sentiment   =   True        \nsentiment           =   []          #average sentiment of each chunck of text \nave_window_size     =   250         #size of scanning window for moving average\n                    \n\n#OUTPUT FILE\noutput='transactions.txt'\nif os.path.exists(output): os.remove(output)\n\n#INITIALIZE\nlemmatizer  =   WordNetLemmatizer()\nps          =   PorterStemmer()\nsia         =   SentimentIntensityAnalyzer()\n\n#ADD MORE\nstopwords   =   stopwords.words('english')\nadd=['mr','mrs','wa','dr','said','back','could','one','looked','like','know','around','dont']\nfor sp in add: stopwords.append(sp)\n\nRead in the csv file:\n\ndf=pd.read_csv(\"free_mtr_text_data.csv\")\nfreeway_list=list(df.loc[df.label==\"freeway\",\"text\"])\nmetro_list=list(df.loc[df.label==\"metro\",\"text\"])"
  },
  {
    "objectID": "index.html#data-cleaning-and-sentiment-analysis",
    "href": "index.html#data-cleaning-and-sentiment-analysis",
    "title": "ARM and Networking",
    "section": "1.3 Data cleaning and sentiment analysis",
    "text": "1.3 Data cleaning and sentiment analysis\nIn this part, I will clean the data and also compute the average sentiment of text data that labeled “freeway” or “metro” in order to compare the sentiment of two groups of tweets.\nFirst define a funtion to further clean the text data and output the average sentiment value of all the tweets about “freeway”.\n\ndef read_and_clean(file):\n    global sentiment \n\n    #CLEAN AND LEMMATIZE\n    keep='0123456789abcdefghijklmnopqrstuvwxy';\n\n    new_sentences=[]; vocabulary=[]\n    for sentence in file:\n        new_sentence=''\n\n        # REBUILD LEMITIZED SENTENCE\n        for word in sentence.split():\n            \n            #ONLY KEEP CHAR IN \"keep\"\n            tmp2=''\n            for char in word: \n                if(char in keep): \n                    tmp2=tmp2+char\n                else:\n                    tmp2=tmp2+' '\n            word=tmp2\n            \n            tmp=lemmatizer.lemmatize(word)\n            new_word=tmp\n\n            #REMOVE WHITE SPACES\n            new_word=new_word.replace(' ', '')\n\n            #BUILD NEW SENTANCE BACK UP\n            if( new_word not in stopwords):\n                if(new_sentence==''):\n                    new_sentence=new_word\n                else:\n                    new_sentence=new_sentence+','+new_word\n                if(new_word not in vocabulary): vocabulary.append(new_word)\n\n        #SAVE (LIST OF LISTS)       \n        new_sentences.append(new_sentence.split(\",\"))\n        \n        #SIA\n        if(compute_sentiment):\n            \n            s=sia.polarity_scores(new_sentence.replace(',',' '))\n            sentiment.append([s['neg'],s['neu'],s['pos'],s['compound']])\n            \n        #SAVE SENTANCE TO OUTPUT FILE\n        if(len(new_sentence.split(','))>2):\n            f = open(output, \"a\")\n            f.write(new_sentence+\"\\n\")\n            f.close()\n\n    sentiment=np.array(sentiment)\n    print(\"TOTAL AVERAGE SENTIMENT:\",np.mean(sentiment,axis=0))\n    print(\"VOCAB LENGTH\",len(vocabulary))\n    return new_sentences\n\ntransactions=read_and_clean(freeway_list)\nprint(transactions[0:5])\n\nTOTAL AVERAGE SENTIMENT: [ 0.10607972  0.71527322  0.17864706 -0.01102825]\nVOCAB LENGTH 7949\n[['boenau', 'rfsdfhsfbhwsfgb', 'least', 'ca', 'expressway', 'might', 'limited', 'access', 'still', 'grade', 'inte', 'http', 'co', 'dibmsr9b5b'], ['acakamadu', 'sudden', 'urge', 'go', 'san', 'jose', 'talk', 'cultural', 'impact', 'song', 'unfortunately', 'san', 'jo', 'http', 'co', 'nujnntbhx0'], ['alinaaai', 'yes', 'girl', 'freeway', 'system', 'wack', 'lol'], ['multi', 'adsbx', 'circling', 'alert', 'time', 'sat', 'dec', '10', '18', '26', '41', '2022', 'n962ms', 'likely', 'circling', 'fl23', '4nm', 'http', 'co', 'dfrvrvjiqt'], ['ghettosmosh', 'almost', 'crashed', 'fucking', 'whip', 'freeway', 'listening', 'legend', 'plug', 'feel', 'owed', 'methinks']]\n\n\nAccording to the average sentiment vector, negative coefficient is 0.11 while positive coefficient is 0.18. At the same time, neutral coefficient is 0.72, which is much higher than others.\nThen compute the sentiment vector of tweets about “metro”\n\nsentiment           =   []\ntransactions1=read_and_clean(metro_list)\nprint(transactions1[0:5])\n\nTOTAL AVERAGE SENTIMENT: [0.06102724 0.8457798  0.0931941  0.06118218]\nVOCAB LENGTH 4436\n[['downed', 'plonk', 'calais', 'bangkok', 'spanish', 'language', 'tickle', 'ear', 'baroque', 'least', 'metro', 'http', 'co', 'tnoj5k3fg'], ['rt', 'lolita', 'tnie', 'railway', 'okaying', 'traffic', 'amp', 'power', 'block', 'bengaluru', 'metro', 'start', 'process', 'erecting', 'open', 'web', 'girder', '65'], ['rt', 'metroopinion', 'another', 'knockout', 'defeat', 'call', 'growing', 'sack', 'gareth', 'southgate', 'evolution', 'revolution', 'solution'], ['rt', 'metrouk', 'breaking', 'reported', 'national', 'grid', 'fired', 'two', 'emergency', 'use', 'coal', 'generator', 'amid', 'cold', 'weather', 'http', 'co'], ['rt', 'mahilmass', 'varisu', 'metro', 'train', 'promotion', 'http', 'co', '2elrimnwar']]\n\n\nFrom this result, we can find that the size relation of negative, positive and neutral coefficients is the same as “freeway”. However, the negative and positive coefficients are smaller while neutral coefficent gets even larger. This means that people include more emotion to the tweets about “freeway” than about “metro”. This possibly because people may encounter traffic jams and car accidents on the freeway but not in metro.\nNext, use moving average plot to visualize the sentiment value among all the tweets about “freeway”:\n\ndef moving_ave(y,w=100):\n\n    l=len(y)\n    mean = []\n    i=0\n    while(i+w<=l):\n        mean.append(np.mean(y[i:i+w]))\n        i=i+1\n    return mean\n\nneg=moving_ave(sentiment[:,0], ave_window_size)\npos=moving_ave(sentiment[:,2], ave_window_size)\n\nneg=(neg-np.mean(neg))/np.std(neg)\npos=(pos-np.mean(pos))/np.std(pos)\n\nfig,axes=plt.subplots(1,1,figsize=(9,7))\nFS=15\nplt.plot(neg, label = 'neg')\nplt.plot(pos, label = 'pos')\nplt.legend(loc='upper left')\nplt.title('sentiment of tweets about \"freeway\"', fontsize=FS)\nplt.xlabel('Chronological order of tweets', fontsize=FS)\nplt.ylabel('sentiment', fontsize=FS)\nplt.show()\n\n\n\n\nWe can see that the average sentiment fluctuates by time."
  },
  {
    "objectID": "index.html#arm-and-networking-modeling",
    "href": "index.html#arm-and-networking-modeling",
    "title": "ARM and Networking",
    "section": "1.4 ARM and networking modeling",
    "text": "1.4 ARM and networking modeling\nFirst define a function to re-format the output:\n\ndef reformat_results(results):\n\n    #CLEAN-UP RESULTS \n    keep=[]\n    for i in range(0,len(results)):\n        # print(\"=====================================\")\n        # print(results[i])\n        # print(len(list(results[i])))\n        for j in range(0,len(list(results[i]))):\n            # print(results)\n            if(j>1):\n                for k in range(0,len(list(results[i][j]))):\n                    if(len(results[i][j][k][0])!=0):\n                        #print(len(results[i][j][k][0]),results[i][j][k][0])\n                        rhs=list(results[i][j][k][0])\n                        lhs=list(results[i][j][k][1])\n                        conf=float(results[i][j][k][2])\n                        lift=float(results[i][j][k][3])\n                        keep.append([rhs,lhs,supp,conf,supp*conf,lift])\n                        # keep.append()\n            if(j==1):\n                supp=results[i][j]\n\n    return pd.DataFrame(keep, columns =[\"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"])\n\nThen define a utility function to convert to NetworkX object:\n\ndef convert_to_network(df):\n    print(df)\n\n    #BUILD GRAPH\n    G = nx.DiGraph()  # DIRECTED\n    for row in df.iterrows():\n        # for column in df.columns:\n        lhs=\"_\".join(row[1][0])\n        rhs=\"_\".join(row[1][1])\n        conf=row[1][3]; #print(conf)\n        if(lhs not in G.nodes): \n            G.add_node(lhs)\n        if(rhs not in G.nodes): \n            G.add_node(rhs)\n\n        edge=(lhs,rhs)\n        if edge not in G.edges:\n            G.add_edge(lhs, rhs, weight=conf)\n\n    return G\n\nDefine another function to plot the NetworkX object:\n\ndef plot_network(G):\n    #SPECIFIY X-Y POSITIONS FOR PLOTTING\n    pos=nx.random_layout(G)\n\n    #GENERATE PLOT\n    fig, ax = plt.subplots()\n    fig.set_size_inches(15, 15)\n\n    #assign colors based on attributes\n    weights_e   = [G[u][v]['weight'] for u,v in G.edges()]\n\n    #SAMPLE CMAP FOR COLORS \n    cmap=plt.cm.get_cmap('Blues')\n    colors_e    = [cmap(G[u][v]['weight']*10) for u,v in G.edges()]\n\n    #PLOT\n    nx.draw(\n    G,\n    edgecolors=\"black\",\n    edge_color=colors_e,\n    node_size=2000,\n    linewidths=2,\n    font_size=8,\n    font_color=\"white\",\n    font_weight=\"bold\",\n    width=weights_e,\n    with_labels=True,\n    pos=pos,\n    ax=ax\n    )\n    ax.set(title='Dracula')\n    plt.show()\n\nTrain the ARM model and print the result:\n\nprint(\"Transactions:\",pd.DataFrame(transactions))\nresults = list(apriori(transactions, min_support = 0.05, min_confidence=0.05, min_length=1, max_length=5))   \npd_results=reformat_results(results)\nprint(len(results))\n\nTransactions:                0                1         2         3             4        5   \\\n0          boenau  rfsdfhsfbhwsfgb     least        ca    expressway    might   \n1       acakamadu           sudden      urge        go           san     jose   \n2        alinaaai              yes      girl   freeway        system     wack   \n3           multi            adsbx  circling     alert          time      sat   \n4     ghettosmosh           almost   crashed   fucking          whip  freeway   \n...           ...              ...       ...       ...           ...      ...   \n2579           20             east   atlanta       see  neighborhood   people   \n2580         ever            since     first       bmw            90    325is   \n2581     cruising            exact   freeway     1930s        german   toward   \n2582    briannawu             even   walking   grocery         store   across   \n2583   congestion               n1   inbound  elevated       freeway   expect   \n\n             6              7            8          9   ...          12  \\\n0       limited         access        still      grade  ...          co   \n1          talk       cultural       impact       song  ...          jo   \n2           lol           None         None       None  ...        None   \n3           dec             10           18         26  ...      n962ms   \n4     listening         legend         plug       feel  ...        None   \n...         ...            ...          ...        ...  ...         ...   \n2579     priced    eliminating  interchange  construct  ...  f0kmfq5tiv   \n2580   obsessed         shadow          car    creates  ...          co   \n2581    fascism       democrat      setting   obstacle  ...   7tveaat5g   \n2582     street       requires      playing       game  ...        four   \n2583      delay  booefreeroads         http         co  ...        None   \n\n              13        14          15    16    17    18          19    20  \\\n0     dibmsr9b5b      None        None  None  None  None        None  None   \n1           http        co  nujnntbhx0  None  None  None        None  None   \n2           None      None        None  None  None  None        None  None   \n3         likely  circling        fl23   4nm  http    co  dfrvrvjiqt  None   \n4           None      None        None  None  None  None        None  None   \n...          ...       ...         ...   ...   ...   ...         ...   ...   \n2579        None      None        None  None  None  None        None  None   \n2580  wa9jva0cwi      None        None  None  None  None        None  None   \n2581        None      None        None  None  None  None        None  None   \n2582        http        co   0w9iudhvv  None  None  None        None  None   \n2583        None      None        None  None  None  None        None  None   \n\n        21  \n0     None  \n1     None  \n2     None  \n3     None  \n4     None  \n...    ...  \n2579  None  \n2580  None  \n2581  None  \n2582  None  \n2583  None  \n\n[2584 rows x 22 columns]\n19\n\n\n\nG=convert_to_network(pd_results)\nplot_network(G)\n\n                rhs              lhs      supp      conf  supp x conf  \\\n0             [car]        [freeway]  0.053019  0.706186     0.037441   \n1         [freeway]            [car]  0.053019  0.067289     0.003568   \n2              [co]        [freeway]  0.323529  0.668265     0.216204   \n3         [freeway]             [co]  0.323529  0.410609     0.132844   \n4              [co]           [http]  0.483746  0.999201     0.483359   \n5            [http]             [co]  0.483746  0.996810     0.482203   \n6              [co]           [lane]  0.054954  0.113509     0.006238   \n7            [lane]             [co]  0.054954  0.706468     0.038823   \n8         [freeway]           [http]  0.323916  0.411100     0.133162   \n9            [http]        [freeway]  0.323916  0.667464     0.216203   \n10        [freeway]           [lane]  0.068498  0.086935     0.005955   \n11           [lane]        [freeway]  0.068498  0.880597     0.060320   \n12        [freeway]             [rt]  0.077786  0.098723     0.007679   \n13             [rt]        [freeway]  0.077786  0.661184     0.051431   \n14        [freeway]        [traffic]  0.119195  0.151277     0.018031   \n15        [traffic]        [freeway]  0.119195  0.950617     0.113309   \n16           [http]           [lane]  0.055341  0.114035     0.006311   \n17           [lane]           [http]  0.055341  0.711443     0.039372   \n18             [co]  [freeway, http]  0.323142  0.667466     0.215687   \n19        [freeway]       [co, http]  0.323142  0.410118     0.132526   \n20           [http]    [co, freeway]  0.323142  0.665869     0.215171   \n21    [co, freeway]           [http]  0.323142  0.998804     0.322756   \n22       [co, http]        [freeway]  0.323142  0.668000     0.215859   \n23  [http, freeway]             [co]  0.323142  0.997611     0.322370   \n24             [co]     [http, lane]  0.054954  0.113509     0.006238   \n25           [http]       [co, lane]  0.054954  0.113238     0.006223   \n26           [lane]       [co, http]  0.054954  0.706468     0.038823   \n27       [co, http]           [lane]  0.054954  0.113600     0.006243   \n28       [co, lane]           [http]  0.054954  1.000000     0.054954   \n29     [lane, http]             [co]  0.054954  0.993007     0.054569   \n\n        lift  \n0   0.896259  \n1   0.896259  \n2   0.848132  \n3   0.848132  \n4   2.058959  \n5   2.058959  \n6   1.459243  \n7   1.459243  \n8   0.847116  \n9   0.847116  \n10  1.117614  \n11  1.117614  \n12  0.839145  \n13  0.839145  \n14  1.206481  \n15  1.206481  \n16  1.466003  \n17  1.466003  \n18  2.060612  \n19  0.847796  \n20  2.058141  \n21  2.058141  \n22  0.847796  \n23  2.060612  \n24  2.051103  \n25  2.060606  \n26  1.460410  \n27  1.460410  \n28  2.060606  \n29  2.051103"
  }
]