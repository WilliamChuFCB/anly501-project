{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad687e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tweepy\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "from langdetect import detect\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = open(\"api-keys.json\")\n",
    "input=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79031037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# GET STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# INITALIZE STEMMER+LEMITZIZER+SIA\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ec0f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD KEYS INTO API\n",
    "consumer_key=input[\"consumer_key\"]    \n",
    "consumer_secret=input[\"consumer_secret\"]    \n",
    "access_token=input[\"access_token\"]    \n",
    "access_token_secret=input[\"access_token_secret\"]    \n",
    "bearer_token=input[\"bearer_token\"]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209e7b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.api.API object at 0x7fb9117da700>\n",
      "monk_digi\n"
     ]
    }
   ],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "print(api)\n",
    "print(api.verify_credentials().screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35270b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_more(query,tweets_num):\n",
    "    num_tweets_collected=0\n",
    "    searches=[]\n",
    "    while num_tweets_collected<tweets_num:\n",
    "        if len(searches)==0:\n",
    "            search_results = api.search_tweets(query,lang=\"en\", count=100)\n",
    "        else:\n",
    "            search_results = api.search_tweets(query,lang=\"en\", count=100,max_id=max_id_next)\n",
    "        num_tweets_collected+=len(search_results)\n",
    "        max_id_next=int(search_results[-1]._json[\"id_str\"])-1\n",
    "\n",
    "        for i in range(len(search_results)):\n",
    "            searches.append(search_results[i])\n",
    "    return(searches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d5f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sh_to_list(searches):\n",
    "    texts=[]\n",
    "    for i in range(len(searches)):\n",
    "        texts.append(searches[i]._json[\"text\"])\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcb04a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquire data from twitter\n",
    "\n",
    "sh_free=search_more(\"freeway\",3000)\n",
    "sh_free_list=list(set(sh_to_list(sh_free)))\n",
    "sh_mtr=search_more(\"metro\",3000)\n",
    "sh_mtr_list=list(set(sh_to_list(sh_mtr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a35dc1f",
   "metadata": {},
   "source": [
    "Raw text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d773d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_free.to_csv('tweet_freeway.csv',index=False)\n",
    "df_metro.to_csv('tweet_metro.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c502b",
   "metadata": {},
   "source": [
    "Clean the text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fc35ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text data\n",
    "\n",
    "def clean_string(text):\n",
    "    #FILTER OUT UNWANTED CHAR\n",
    "    new_text=\"\"\n",
    "    keep=\" abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "    for character in text:\n",
    "        if character.lower() in keep:\n",
    "            new_text+=character.lower()\n",
    "        else: \n",
    "            new_text+=\" \"\n",
    "    text=new_text\n",
    "\n",
    "    #FILTER OUT UNWANTED WORDS\n",
    "    new_text=\"\"\n",
    "    for word in nltk.tokenize.word_tokenize(text):\n",
    "        if word not in nltk.corpus.stopwords.words('english'):\n",
    "            #lemmatize \n",
    "            tmp=lemmatizer.lemmatize(word)\n",
    "            word=tmp\n",
    "            if len(word)>1:\n",
    "                if word in [\".\",\",\",\"!\",\"?\",\":\",\";\"]:\n",
    "                    #remove the last space\n",
    "                    new_text=new_text[0:-1]+word+\" \"\n",
    "                else: #add a space\n",
    "                    new_text+=word.lower()+\" \"\n",
    "    text=new_text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75f39bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sh_free_list)):\n",
    "    sh_free_list[i]=clean_string(sh_free_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a719c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sh_mtr_list)):\n",
    "    sh_mtr_list[i]=clean_string(sh_mtr_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f276fd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text    label\n",
      "0     boenau rfsdfhsfbhwsfgb least ca expressway mig...  freeway\n",
      "1     zacakamadu sudden urge go san jose talk cultur...  freeway\n",
      "2           alinaaaziz yes girl freeway system wack lol  freeway\n",
      "3     multi adsbx circling alert time sat dec 10 18 ...  freeway\n",
      "4     ghettosmosh almost crashed fucking whip freewa...  freeway\n",
      "...                                                 ...      ...\n",
      "2652  20 east atlanta see neighborhood people priced...  freeway\n",
      "2653  ever since first bmw 90 325is obsessed shadow ...  freeway\n",
      "2654  cruising exact freeway 1930s german toward fas...  freeway\n",
      "2655  briannawu even walking grocery store across st...  freeway\n",
      "2656  congestion n1 inbound elevated freeway expect ...  freeway\n",
      "\n",
      "[2657 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "tmp1=[]\n",
    "for i in range(0,len(sh_free_list)):\n",
    "    tmp1.append([sh_free_list[i],\"freeway\"])\n",
    "df1=pd.DataFrame(tmp1)\n",
    "df1=df1.rename(columns={0: \"text\", 1: \"label\"})\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f100c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  label\n",
      "0     congratulation district metro gold south honor...  metro\n",
      "1     rt kumardprince1 ktrtrs ltmhyd best investment...  metro\n",
      "2     rt chip vitaliy even going hell keep going ina...  metro\n",
      "3     water stagnation 47th street 9th avenue ashokn...  metro\n",
      "4     rt drdpgoel board nagpur metro hon ble prime m...  metro\n",
      "...                                                 ...    ...\n",
      "1360  rt veryrare metro boomin leaving studio making...  metro\n",
      "1361  rt antsharwood interesting morning sydney weat...  metro\n",
      "1362           hihyderabad abmandadapu metro station pl  metro\n",
      "1363  debrobertsabc autopsy georgia2022election requ...  metro\n",
      "1364  rt pugazh98 exclusive varisu sticker getting r...  metro\n",
      "\n",
      "[1365 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "tmp2=[]\n",
    "for i in range(0,len(sh_mtr_list)):\n",
    "    tmp2.append([sh_mtr_list[i],\"metro\"])\n",
    "df2=pd.DataFrame(tmp2)\n",
    "df2=df2.rename(columns={0: \"text\", 1: \"label\"})\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ba6c513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text    label\n",
      "0     boenau rfsdfhsfbhwsfgb least ca expressway mig...  freeway\n",
      "1     zacakamadu sudden urge go san jose talk cultur...  freeway\n",
      "2           alinaaaziz yes girl freeway system wack lol  freeway\n",
      "3     multi adsbx circling alert time sat dec 10 18 ...  freeway\n",
      "4     ghettosmosh almost crashed fucking whip freewa...  freeway\n",
      "...                                                 ...      ...\n",
      "1360  rt veryrare metro boomin leaving studio making...    metro\n",
      "1361  rt antsharwood interesting morning sydney weat...    metro\n",
      "1362           hihyderabad abmandadapu metro station pl    metro\n",
      "1363  debrobertsabc autopsy georgia2022election requ...    metro\n",
      "1364  rt pugazh98 exclusive varisu sticker getting r...    metro\n",
      "\n",
      "[4022 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.concat([df1,df2])\n",
    "print(df)\n",
    "df.to_csv('free_mtr_text_data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
